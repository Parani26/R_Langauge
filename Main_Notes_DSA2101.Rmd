---
title: "R notes"
author: "Parani"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, include = TRUE,
                      fig.align = "center",  out.width = "80%")
```

###### To do when obtaining a dataset
- data(dataset) # load dataset
- set.seed()
- head(dataset, 5)
- names(dataset)
- clear environment
- glimpse(dataset)
- class(dataset)
- asfactor, asnumeric relevant columns

- %>% head()
- (file in src, data in data, to access data is ../data/xxxxx.csv), do not setwd or getwd for rmarkdown
- chcek if you need to group or order variables first
- Identify the required dataset that you need to plot
- use ungroup() at the end of a series of grouped operations



###### R markdown
- Run this line #install.packages(c("rmarkdown", "knitr"))
- Turn on knit on save mode

###### Miscellaneous
- Use command + enter to run the code without copying and pasting
- To do upon receiving data
attach, dim, head and as factor relevant columns
- use ?[Command_name] to get help
- Arithmetic + - * / ^
- Useful R packages - #install.packages(c("tidyverse", "ggplot2", "stringr"))
- Do not name with "False, Inf, NA, NaN, Null, True" and naming is case sensitive
- use args([function name]) to find the arguements required in that function
- type ?"regular expression" into console to learn more about regular expressions in R

###### Working directory
- Tools -> Global Options -> Set working directory
- Get Absolute working directory # Cannot do for RMarkdown
- Set working directory # Cannot do for RMarkdown
- ".." Goes up one heirachy
```{r}
# getwd()
# setwd("..")
# getwd()
# setwd("/Users/tech26/Desktop/NUS/ACADEMICS/Y2S1/DSA2101/My_notes")
# Can access files by doing "../../../data/height.csv" 
```

#### BASE R
- work with individual vectors

###### Save, read and remove R objects
```{r}
tibet <- c(1, 2, 3, 4, 5)
saveRDS(tibet, file = "/Users/tech26/Desktop/NUS/ACADEMICS/Y2S1/DSA2101/My_notes/tibet.rds")
tibet_1 <- readRDS("/Users/tech26/Desktop/NUS/ACADEMICS/Y2S1/DSA2101/My_notes/tibet.rds")
x <- 1:5
rm(list = c("x")) # Remove x from workspace
rm(list = ls()) # Remove all objects from workspace
```

###### Logical expressions
- < > <= >= == != will return TRUE or FALSE
- Intersection of TRUE and FALSE becomes FALSE
- Union of TRUE and FALSE becomes TRUE
```{r}
x <- 1:5
y <- x <= 3; y
z <- x >= 3; z
y & z 
y | z 
!y 
```

###### Classes
```{r}
a <- 2.7
b <- "hello"
c <- TRUE
class(a)
class(b)
class(c)
```

###### Test for class
```{r}
is.numeric(a)
is.logical(c)
is.character(c)
is.factor(a)
```

###### Coercing into different classes
```{r}
as.numeric(c)
as.logical(b)
as.character(a)
as.factor(c)
```

###### Dates
```{r}
# install.packages("lubridate")
# yr_month = paste(Year,Month, sep = "-")) # convert 1841 and 1 to 1841-1
library(lubridate, warn.conflicts = FALSE)

d1 <- as.Date("2024/08/15", "%Y/%m/%d"); d1 # time in utc
d2 <- as.Date("2024/08/16", "%Y/%m/%d"); d2 # time in utc
d2-d1 # to get the time difference
class(d1)

today <- Sys.Date() ; today # to obtain today's date
s1 <- seq(today - 100, today, by = "1 week") ; s1 # creates a sequence of dates starting from 100 days before today to today, incrementing by one week at a time
s1[1:3]

weekdays(d1, abbreviate = FALSE) # To find day in a date (full)
weekdays(d1, abbreviate = TRUE) # To find day in a date (short form)
months(d1, abbreviate = FALSE) # To find month in a date (full)
months(d1, abbreviate = TRUE) # To find month in a date (short form)

target_month <- month.name[5:9];target_month # converts numbers to months

yr_month <- "1841-2"
ym(yr_month) # another way to create date, 1st of the month and year

date <- c(ymd("1963-01-01"), ymd("1963-04-01"), ymd("1963-07-01"), ymd("1963-10-01")); date
quarter = as.factor(paste0("Q",quarter(date))); quarter # Obtain the quarter of the year date falls in
```

###### Vector creation
- all elements of vector have to same class
- class(vector name) to check
```{r}
number<- c(2,4,6,8,10); number # creating a vector of numbers, c is a combine function that takes comma seperated numbers or strings and joins them into a vector

insh <- c(9,3,5,7,1)
string<- c("weight", "height", "gender"); string # creating a vector of strings/characters
logic<- c(T, T, F, F, T); logic # creating a Boolean vector (T/F)
length(logic) # length of the vector
v <- c(1:6); v # creating vector of 1 to 6
shiv <- 22:26; shiv # creating vector of 22 to 26
par <- 0.5^shiv; par # raising power
shiv[2] # access second element
par[3:5] # access third to fifth element
par[length(par)] # access the last element
shiv[1] <- -3
shiv <- shiv + 2; shiv # plus two applied to all elements
all <- c(par, par) # saving double of previous vector into another vector
hap <- 2*shiv + par + 1 # vector operations
all + shiv  # When adding vectiors of different lengths, R "recycles" the shorter vector (shiv) to match the length of all. But longer object length has to be a multiple of shorter object length

number.2<- numeric(3); number.2 # creating a vector of zeros

new.number<- c(number, number.2); new.number
new.number1<- append(number, number.2);new.number1

# rep(a,b): replicate the item a by b times where a could be a number or a vector
number.3<- rep(2,3); number.3
number.3<- rep(c(1,2),3); number.3 # need to have the c
new.string<- rep(string,2); new.string

sequence1 <- seq(from=2, to=10, by=2);sequence1 # vector of 2,4,6,8,10
sequence2 <- seq(2,10,2);sequence2 # preferred # vector of 2,4,6,8,10
sequence3 <- seq(from=2, to=10, length = 5);sequence3 <- # vector of 2,4,6,8,10
sequence4 <- seq(10);sequence4 # a sequence from 1 up to 10, distance by 1 (automatic)

m <- matrix(v, nrow=2, ncol=3); m # creates a matrix arranging data into the matrix created, by default arrange by columns
m <- matrix(v, nrow=2, ncol=3, byrow=T); m # to fill the matrix by rows
m <- matrix(v, nrow=2, ncol=3, byrow=F); m # to fill the matrix by columns #automatic if nth specified
mat <- matrix(1:12, nrow = 4, byrow = TRUE); mat # fill rows first
dim(mat) # to obtain the dimension
mat[2,3] # select entry at the 2nd row and 3rd column
mat[2,] # select elements in the 2nd row
mat[c(1,2),] # select elements in the 1st row and 2nd rows

a <- c(1,2,3,4)
b <- c(5,6,7,8)
ab_row <- rbind(a,b); ab_row # bind by row
ab_col <- cbind(ab_row, c(9,10)); ab_col # bind by column
```

###### List
- collection of objects that can be of different classes and lengths
- list will print out the data in 1D (vertically)
```{r}
list.1 <- list(10.5, 20, TRUE, "Daisy"); list.1
mylist <- list(A = 1,
                 B = c(1,2),
                 C = c(TRUE, FALSE, TRUE),
                 D = matrix(1:6, nrow =3)); mylist # First way to make lists
mylist[[3]] # Extracts the elements within the element
mylist[3] # Extract the third element which is a list
mylist$C # Extract the element which within the element
# class() and names() work on lists as well
x = c(2,4,6,8); x # length 4
y = c(T, F, T); y # length 3
list.2 = list(Hi = x, Hello = y); list.2 # assign names to list members, second way to make lists
# referencing in list
list.2[1] # reference by index, indexing starts from 1 in R, 
list.2$Hi # reference by name
```

###### Common commands
```{r}
number
max(number) # maximum value of vector
min(number) # minimum value of vector
sum(number) # total of all the values in x
mean(number) # arithmetic average values in x
range(number) # min(x) and max(x)
cor(number, insh)# correlation bw vectors x and y # only for quantitative variables
sort(insh) # sorted version of x
```

###### Functions apply, sapply, tapply, lapply
- apply() applies a function across rows or columns of a matrix or a dataframe
- sapply() applies a function across elements of a list and returns a vector or a matrix if not possible then list, arguments (vectors/list, function)
- lapply() applies a function across elements of a list and returns a list, arguments (vectors/list, function)
- tapply() applies an function on a subset of vector/data broken down by a given factor variable/levels, arguments (vectors/list, index, function)

e.g tapply(price, city, mean)
e.g tapply(price, list(city, region),
function(x) round(mean(x), 2))

tapply(Temp, Season, summary) # For summary statistics


```{r}
my_list <- list(a = 1:5, b = 6:10, c = 11:15); my_list
result <- lapply(my_list, sum); result
result2 <- sapply(my_list, sum); result2

values <- c(5, 10, 15, 20, 25, 30)
groups <- factor(c("A", "A", "B", "B", "C", "C"))
result <- tapply(values, groups, mean); result # first argument is the value vector, second is the factor vector, and last is the function

my_mat <- matrix(1:30, nrow = 10, byrow = FALSE); my_mat
apply(my_mat, MARGIN = 2, mean) # (arguements: dataset, MARGIN = 1 --> ROWS, MARGIN = 2 --> COLUMNS, FUNCTION TO BE APPLIED)
apply(my_mat, MARGIN = 1, mean)
apply(my_mat, MARGIN = 2, function(x) sum(x) + 3) # setting your own function
apply(my_mat, MARGIN = 2, function(x) c(mean(x), median(x))) # Carrying out two functions us c(,) # First row of the result will be mean and second row will be result 
```
        

###### String functions/ Numeric functions
- tolower(): Converts text to lowercase.
- str_remove(): Removes specified patterns from strings.
- paste(): Concatenates strings with a specified separator.
- case_when(): Applies conditional logic to create new variables.
- ifelse(): Applies simple conditional logic for creating new variables.
- cumsum(): Calculates the cumulative sum of a vector.
- replace_na(): Replaces missing values (NA) with specified values.
- na_if(): Converts specified values to NA.
```{r}
# install.packages("stringr")
library(stringr)

fruits <- c("apple", "bannana", "orange")

str_detect(fruits, "an") # to detect presence of pattern in string elements
str_detect(fruits, "an", negate = TRUE) # to detect absence of pattern in string elements
str_detect(fruits, "^a") # to detect string elements that begins with a
str_detect(fruits, "a$") # to detect whether string ends with a
str_detect(fruits, "[0-9]") # to detect whether a string contains any digits
str_replace(fruits, "[aeiou]", "X") # to replace first match of any letter with X
str_replace_all(fruits, "[aeiou]", "?") # to replace all matches of any letter with X

str_length(fruits) # gives number of characters of each string element
str_c(fruits) # collapses multiple character vectors into a single character string (no more comma)
str_sub(fruits, 1, 3) # subset each string from pos to pos
str_sub(fruits, -3, -1)
str_count(fruits, "[aeiou]") # count following characters in each string element
# mutate(grade = as.numeric(str_remove(grade, "th")))

file_names <- c("andreazhang_S13774_4717352_Week 4 ANDREA ZHANG.Rmd",
"chettyraj_S11993_4717353_wk4_Chetty.RMD",
"chooclarence_S10609_4717354_tut_week_4.RMD",
"suryaprimyan_S10197_4717355_Surya.Rmd")
split_names <- str_split_fixed(file_names, "_", 4); split_names # convert text into a matrix, # splits strings into a fixed number of parts


```

###### Dataframe in R
- special case of lists
- data frame columns can be of different class types
```{r}
# creating data frame, creating synthetic data
budget_cat <- c("Manpower", "Asset", "Other")
amount <- c(519.4, 38.0, 141.4)
op_budget <- data.frame(budget_cat, amount); op_budget
op_budget[, "budget_cat"] # selecting budget category
op_budget$budget_cat # selecting budget category $ is accessor operator

# reading data frame
data1<-read.csv("crab.txt", sep = "", header = FALSE) 
data = data1[,-(1:3)] # REMOVE THE FIRST THREE COLUMNS (useless for analysis)

# reading data sets in base R
data(cars) # data() load and list datasets available in R packages
str(cars) # structure of data frame
names(cars) # to see the column names
head(cars) # see the first 6 rows of the data frame

# number of rows and columns
nrow(data1) # number of rows
ncol(data1) # number of columns
dim(data1) #  Number of rows and columns

# Reading data
# data \ seperated
dat = read.table("Colleges.txt",header =TRUE,sep= "\t") # read.table
# data , seperated
data1<-read.csv("crab.txt",sep = "", header = TRUE) # header True reads the first line of data as headers # read.csv
data1<-read.csv("crab.txt", sep = "", header = FALSE) # header false reads the headers as data

# to rename variables
varnames <- c("Subject", "Gender", "CA1", "CA2", "HW") # first way to rename
data2<-read.table("ex_1.txt", header = FALSE, col.names = varnames); data2
data4<-read.table("ex_1.txt", header = FALSE); data4
names(data4)[1:5] = c("Subject", "Gender", "CA1", "CA2", "HW") # first way to rename column name 
names(data4)[4] = "status" # second way to rename column name

# reading specific data
attach(data1) # then dont need to read each variable
data1[1:8,] # first 8 rows 
data1[(nrow(data1) - 7):nrow(data1),] # last 8 rows
data1[,1:3] # first 3 column
data1[,(ncol(data1) - 2):ncol(data1)] # last 3 columns
data1[,1] # first column
data1[3,3] # value at 3rd row & 3rd column
data1[3,4] # value at 3rd row & 4th column
names(data1) # names of columns 
head(data1) # header and first 6 data sets

bankdata = read.csv("bank-sample.csv", header=TRUE)
head(bankdata[,c(9,16,17)]) # to view columns 9, 16, 17
table(bankdata$job) # To get summaries 

# Selecting rows of data of specific data
attach(data4)
data4[data4$Gender == "M",] # to obtain all the rows (observations) whose gender = M
data4[Gender == "M" & CA1 > 85,] # to obtain all the rows (observations) whose gender = M and CA2 > 85

# add all values in the table
sum(table(data4$Gender))

# Factors
x1 <- c("Junior", "Freshmen", "Sophomores", "Seniors")
x1 <- factor(x1)
x1_levels <- c("Freshmen", "Sophomores", "Junior", "Seniors")
x2 <- factor(x1, levels = x1_levels, ordered = TRUE); x2 # If left to by default will be in alphabatical order

# Converting categorical data to numerical
titanic <- read.csv("Titanic.csv", header = TRUE, sep = ",")
attach(titanic)
sur = ifelse(Survived == 'Yes', 1, 0) 
sur = as.factor(sur) 
titanic$sur = sur ; head(titanic) # changing data itself
# Be careful about using dot if your changing a variable

# drop a few columns to simplify the tree
banktrain <- read.csv("bank-sample.csv", header = TRUE, sep = ",")
drops<-c("age", "balance", "day", "campaign", 
         "pdays", "previous", "month", "duration")
banktrain <- banktrain [,!(names(banktrain) %in% drops)]
head(banktrain)

# SCALING THE INPUT FEATURES
caravan <- read.csv("caravan.csv", header = TRUE, sep = ",") 

str(caravan)

standardized.X = scale(caravan[,-87]) # scaling all the dataset, except the last column, column 86 = RESPONSE, used to compare values of different scales like income and age
# if u create a model based on standardised values make sure to standarise the test case as well before running model on it
data = read.csv("data-midterm.csv")
data = data[,4:8] 
names(data)[2] = "X1"
names(data)[3] = "X2"
names(data)[4] = "X3"
names(data)[5] = "X4"
new = data.frame(X1 = 83, X2 = 57, X3= 2, X4 = 3)
standard = scale(rbind(data[,2:5], new) ) 
# dont standardise response varibales only input variables

# GENERATING RANDOM NUMBERS
set.seed(2101) # For reproduciblity
sample(1:6, size = 1) # Randomly select one number from  vector 1:6
sample(1:6, size = 2, replace = TRUE) # Sample with replacement

# IF/ELSE LOOP
w <- 5
if (w <= 5){
  w1 <- 2
} else {
  w1 <- 10
}
w1

#  WHILE LOOP
# when you know stopping condition, more flexible, can result in infinite loop be careful
x = 1
while(x<=3) {print("x is less than 4")
             x = x+1}

set.seed(2101)
game_counter <- 0
while(TRUE) {
  A <- sample(1:6, size = 1)
  B <- sample(1:6, size = 1)
  game_counter <- game_counter + 1
  if (A > B)
    break # Break the loop
}
paste("First win for A occurs at game", game_counter)

x<-0; S<-0 # Find the sum of first 10 integers:
while(x<=10) {S<- S+ x
              x<-x+1}; S

#  FOR LOOP
# When you know no of times to iterate 
S<-0 # Example: find the sum of first 10 integers
set.seed(2101)
results <- rep(0, 1000)
for(i in 1:10){S <-S+i}; S
for(i in 1:1000){
  A <- sample(1:6, size = 1)
  B <- sample(1:6, size = 1)
  
  if (A > B){
    results[i] = "A"
  } else if (A == B) {
    results[i] == "Draw"
  } else {
    results[i] = "B"
  }
}

table(results)

x = c(2, 4, 3, 8, 10) # Find the mean of vector x
l = length(x) 
S = 0
for (i in 1:l){S = S + x[i]}
ave = S/l; ave

x = c(1:100) #Find the sum of all even numbers from 1 up to 100.
S = 0
for (i in 1:length(x)){
  if(x[i]%%2 ==0){S = S + x[i]} else {S = S}
}
print(S)

# defining a function
F1 <- function(down_payment, saved, monthly_return, portion_saved, salary, months){ # arguements defined in (), which can also be defined for user e.g (down_payment = 3, saved = 5)
  while (saved < down_payment){
    months <- months + 1
    saved <- saved + saved * monthly_return + salary * portion_saved
  }
  return(months)
}

F1(1,2,3,4,5,6) # can do F1() if arguments defined in function alr e.g function(down_payment = 1, saved, monthly_return = 2, portion_saved = 3, salary = 4, months = 5) # This is how we call functions

# CONDITIONS WITH if()... else if()... else()
x = c(1:10); # a vector of numbers from 1 to 10
# we want to divide this vector into 3 subsets: 
# a set of all small numbers from 1 to 3
# a set of all medium numbers, from 4 to 7
# a set of large numbers from 8 to 10
S = numeric(0)
M = numeric(0)
L = numeric(0) 
for (i in 1:length(x)){
  if (x[i] <=3){S = append(S, x[i])} else if (x[i]< 8)
  {M = append(M, x[i])} else {L = append(L, x[i])}
}
print(S)
print(M)
print(L)

# FUNCTION ifelse()
x = c(1:8);x
y = ifelse(x%%2 == 0, "even", "odd"); y # if true first statement else second statement

#  REPEAT LOOP
i <-1 # EXAMPLE: print the first five integers
repeat {
  print (i)
  if(i ==5) { break } # break stops the code # must have
  i <- i+1
 }

S = 0 # Example: obtain the sum of first 5 integers
i <-1
repeat {
 S <-S+i;
 if(i ==5) { break }
 i <- i+1
 }; S
```

######  Basic probaility and statistics

```{r}
sales <- read.csv("yearly_sales.csv")
head(sales)
total = sales$sales_total # naming a specific column / variable
attach(sales)

n = length(total); n # number of observations in that variable
summary(total) # info on min, 1st quart, median, mean, 3rd quart and mx

# More information
range(total)
var(total)
sd(total)
IQR(total)
total[order(total)[1:5]] # The 5 smallest observations
total[order(total)[(n-4):n]] #The 5 largest observations

# HISTOGRAM in FREQUENCY
hist(total, freq=FALSE, main = paste("Histogram of total sales"),
     xlab = "total", ylab="Probability", col = "blue")
lines(density(total), col = "red") # this is the density curve of "total"

# HISTOGRAM WITH DENSITY LINE
hist(total, freq=FALSE, main = paste("Histogram of total sales"),
     xlab = "total", ylab="Probability", 
     col = "blue", ylim = c(0, 0.0045)) # ylim determines the minimum and maximum values that will be displayed on the y-axis of the plot
lines(density(total), col = "red") # this is the density curve of "total"
# xlim = c(0, 3000)) , can add after col = "blue" to limit the x axis


# HISTOGRAM WITH NORMAL DENSITY 
hist(total, freq=FALSE, main = paste("Histogram of Total Sales"),
     xlab = "total sales", ylab="Probability", 
     col = "grey", ylim = c(0, 0.002)) # remove the lim if necssary
y <- dnorm(x, mean(total), sd(total))
x <- seq(0, max(total), length.out=length(x))
lines(x, y, col = "red") # this is the normal density curve

hist(sales_total,prob = TRUE)
x= seq(50,90, length.out = length(sales_total))
y = dnorm(x, mean(sales_total), sd(sales_total))
lines(x,y, col = "red")

# The histogram shows that the sample is unimodal. Compared to the overlaid normal density curve,
# the distribution looks slightly right-skewed. Most of the observations are within a range of 0.5 to 6 
# – there are no observations that are separated from the rest. However, this does not mean there are no outliers.


fev <- read.csv("FEV.csv")
sex = fev$Sex
FEV = fev$FEV
# saving the values of a variable of categories of another variable e.g fev value of male and female
# plot seperate catgories
female = FEV[which(sex==0)] # or FEV[Sex==0]
male = FEV[which(sex==1)] # or FEV[Sex==1]

# comparing histogram models ***
# The shapes of the two histograms are quite different. 
# Both are unimodal, but for females, it is almost symmetrical but for males it is quite right-skewed.
# The median FEV for females is much lower than that for males (2.49 compared to 2.605).
# In addition, the variability in the male group is higher than the variability in the female group. 
# The respective IQR are 1.54 and 1.05.

# Split plot into 2, for spliting into 4 do (2,2)
opar <- par(mfrow=c(1,2)) #arrange a figure which has 1 row and 2 columns (to contain the 2 histograms)
#(2,2) for 4 histograms

hist(female, col = 2, freq= FALSE, main = "Histogram of Female FEV", ylim = c(0,0.52))
hist(male, col = 4, freq= FALSE, main = "Histogram of Male FEV", ylim = c(0,0.52))
par(opar) # Reset back the plot 
opar <- par(mfrow=c(1,1)) # last resort

#numerical summaries on data sets with specific observations
median(female)
IQR(female)
summary(female)
var(female)

# BOX PLOTS
boxplot(total, xlab = "Total Sales", col = "blue")
boxplot(FEV, col = 10, ylab = "FEV", main = "Boxplot of FEV")
outlier = boxplot(total)$out;outlier # get outlier values
length(outlier) #number of outliers

# There are 9 outliers. Check the information of these 9 outliers, ***
# we would see that all these outliers are males, most (8/9) are non-smokers, and they are rather tall.

#get the indexes of outlier points
index = which(total %in% outlier)
index

#info on all outlier
sales[c(index),]

# QQ plot # to check if normally distrubuted
qqnorm(total, main = "QQ Plot", pch = 20)
qqline(total, col = "red")
 
# switch x and y
qqnorm(total, datax = TRUE, main = "QQ Plot",pch = 20)
qqline(total, datax = TRUE, col = "red")
# datax arguement optional

# From the qq plot, on the left tail, the sample quantiles are larger than expected (theoretical quantile) 
# hence the left tail is shorter than normal.
# On the right side, the sample quantiles are larger than expected, hence the right tail is longer than normal.
# Conclude: Combining with the histogram of FEV, it’s clear that the sample of FEV is not normally dis- tributed and quite right skewed.

# CORRELATION COEFFICIENT
order = sales$num_of_orders
cor(total, order) #0.75

# The computed correlation is quite high, and it is clear from the plot 
# that there is a strong positive linear association between the two variables overall.
# The range of FEV for males appears larger than the range for females, as does the range of heights.
# The variability of FEV at lower heights does seem to be slightly less than the variability of FEV 
# at greater heights.

# SCATTER PLOT
plot(order,total) # automatic settings
plot(order,total,pch=20,col="darkblue") # takes arguments x and y which should be vectors of the same length
plot(cars$speed, cars$dist, pch = 19, col = "red", cex = 2, xlab = "Speed (mph)", ylab = "Stopping distance (ft)", main = "Relationship between Sp-eed and Braking")
# cahnging pch lets you change the symbol used
# cex stands for “character expansion”, with a default value of 1.
# cex.axis affects the font size of the axis
# cex.main affects the font size of the title, and so on.
# col for colour, run colors() to see all colours
abline(reg = lm(dist ~ speed, data = cars),col = "gray60", lty = "dashed")
# to add trend line

# BOX PLOTS OF MULTIPLE GROUPs
boxplot(total ~ sales$gender, col = "blue")
#assoc bw gender and sales


# 3 VARIABLES = SCATTER PLOT ADDING LEGEND
# plotting different observations of data
order = sales$num_of_orders
attach(sales)
# plotting data points separately according to categories of a variable
# x first, y second
plot(order,total, type = "n") # a scatter plot with no point added
points(order[gender=="M"],total[gender=="M"],pch = 2, col = "blue") # MALE
points(order[gender=="F"],total[gender=="F"],pch = 20, col = "red") # FEMALE
legend(1,7500,legend=c("Female", "Male"),col=c("red", "blue"), pch=c(20,2))
# (x = 1, y =7500) tells R the place where you want to put the legend box in the plot
# do note on the size of the points since the points added latter will overlay on the points added earlier
# hence, the points added latter should be chosen with smaller size so that they will not cover the points earlier


# scatter plot
x = c(0.4, 0.4, 0)
y = c(1, 0.4, 0.4)
plot(x,y, type = "n", xlab = "FPR", ylab = "TPR", ylim = c(0,1), xlim = c(0,1))
points(0.4,1, pch = 10, col = "red") # sigma = 0.3
points(0.4,0.4, pch = 10, col = "blue") # sigma = 0.6
points(0,0.4, pch = 10, col = "black") # sigma = 0.8
legend(0.6, 0.3, legend = c("sigma = 0.3", "sigma = 0.6", "sigma = 0.8"), 
       col = c("red", "blue","black"), pch = c(10, 10, 10))

# BARPLOT FOR CATEGORICAL VARIABLE
count = table(gender); count # frequency table
barplot(count)
barplot(op_budget$amount,
        border = NA, col = "gray60", names.arg = op_budget$budget) # Can add main for title and xlab and ylab

# PIE CHART
count = table(gender); count # frequency tablex
pie(count)

# CATEGORIZING "ORDER"
order = sales$num_of_orders
# changing the data itself to small and large based on whether more or less than 5
sales$num_of_orders = ifelse(sales$num_of_orders <= 5, "small", "large") #replacing data with small or large based on condition
table(order)

# CONTINGENCY TABLE
table = table(gender,order);table
tab = prop.table(table, "gender");tab # proportion by gender, female / male total prob = 1
tab[1]/(1-tab[1]) # the odds of large order among FEMALES
tab[1]/tab[3] # same
tab[2]/(1-tab[2]) # the odds of large order among MALES
tab[2]/tab[4] # same
OR = (tab[1]/(1-tab[1]))/(tab[2]/(1-tab[2])); OR # 0.76
OR = (tab[1]/(tab[3]))/(tab[2]/(tab[4])); OR # same thing
# it means: the odds of larger orders among females is 0.76 times the odds of large orders among males.
```

###### Randomly choosing 2000 observations to form test and train x and y 
```{r}
# Be careful of 800 vs 200
n = dim(caravan)[1] # sample size = 5822
test = sample(1:n, 2000) # sample a random set of 2000 indexes, from 1:n.
Purchase <- caravan$Purchase
# response already removed from standardized.X
train.X=standardized.X[-test ,] #training set
test.X =standardized.X[test ,]  # test set
train.Y=Purchase[-test] # response for training set
test.Y =Purchase[test] # response for test set


# Or #

# Be careful of 800 vs 200
credit = read.csv("German_credit.csv")
credit[ ,2:5] = lapply(credit[,2:5], scale)
train = sample (1:1000 , 800); #randomly sample a set of 800 indexes in 1:1000
train.data = credit[train,] # 800 data points for the train set
test.data = credit[-train,] # 200 data points for the test set
train.x = train.data[ ,2:5]
test.x = test.data[ ,2:5]
train.y = train.data[ ,1]
test.y = test.data[ ,1]
```

###### Splitting data into train and test sets so that equal number of observations

```{r}
# test data : train data <- 1:4
# test and train data will have equal percentage of positive and negative response
n_folds = 5
folds_negative <- sample(rep(1:n_folds, length.out = dim(data4)[1]/2 )) 
# test data : train data <- 1:4
# test and train data will have equal percentage of positive and negative response
diabetes <- read.csv("diabetes_5050.csv")
n_folds = 5
folds_negative <- sample(rep(1:n_folds, length.out = dim(diabetes)[1]/2 )) 
folds_positive <- sample(rep(1:n_folds, length.out = dim(diabetes)[1]/2 )) 
folds_positive <- sample(rep(1:n_folds, length.out = dim(diabetes)[1]/2 )) 

table(folds_negative)
table(folds_positive)

negative_data = diabetes[1:35346,] # Only negative response data
positive_data = diabetes[35347:70692,] # Only positive response data

acc_2 = numeric(n_folds)
err_2 = numeric(n_folds)
fpr_values_2 = numeric(n_folds)
fnr_values_2 = numeric(n_folds)

# then doing n fold
for (j in 1:n_folds){
  test_nej <- which(folds_negative == j)
  test_pos <- which(folds_positive == j)
  
  train_nej = negative_data[ -test_nej, ]
  train_pos = positive_data[ -test_pos, ]
  test_nej = negative_data[test_nej, ]
  test_pos = positive_data[test_pos, ]
  
  train_knn = rbind(train_nej, train_pos)
  test_knn = rbind(test_nej, test_pos)
  
  # use thsee new train and test to run the model
  # use acc_2,err_2,fpr_values_2,fnr_values_2 to add values in or other performance indicators when neccesary
}

table(folds_negative)
table(folds_positive)

negative_data = diabetes[1:35346,] # Only negative response data
positive_data = diabetes[35347:70692,] # Only positive response data

acc_2 = numeric(n_folds)
err_2 = numeric(n_folds)
fpr_values_2 = numeric(n_folds)
fnr_values_2 = numeric(n_folds)

# then doing n fold
for (j in 1:n_folds){
  test_nej <- which(folds_negative == j)
  test_pos <- which(folds_positive == j)
  
  train_nej = negative_data[ -test_nej, ]
  train_pos = positive_data[ -test_pos, ]
  test_nej = negative_data[test_nej, ]
  test_pos = positive_data[test_pos, ]
  
  train_knn = rbind(train_nej, train_pos)
  test_knn = rbind(test_nej, test_pos)
  
  # use thsee new train and test to run the model
  # use acc_2,err_2,fpr_values_2,fnr_values_2 to add values in or other performance indicators when neccesary
}
```

###### Linear regression
```{r}
# own function in r to derive eqn of model
simple <- function(x , y) {
  beta_1 <- (sum(x*y)- mean (y)* sum (x ))/( sum(x^2)- mean(x)* sum(x));
  beta_0 <- mean(y)- beta_1* mean(x) ;
  return(c( beta_0 , beta_1)) ;
}

simple(x,y) # manual way of calculating beta 0 and beta 1

# MODELLING
x = c( -1, 3, 5)
y = c( -1, 3.5 , 3)
M1 = lm(y~x) # shows beta 0 and beta 1
M1$fitted # y values when x fitted into eqn
# if you fit linear model for probability might get neagtive values: limitation

# also can do
crab <- read.csv("crab.csv")
attach(crab)
M1 = lm(weight~width+spine,data = data)

# Is the fitted model significant?
# The fitted model, M, has F-test for the overall significance of the model 
# with extremely small p-value. Hence, model M is signicant.



# solve to calulate inverse
# t(x) is to tranpose the matrixz
# The %*% operator is used for matrix multiplication or dot product between two matrices

matrix <- function(x, y) {
  beta <- solve(t(x )%*% x )%*% t(x )%*% y
  return( beta )
} # to call use matrix(x,y)

# PREDICTING
new = data.frame(life_expectancy = 83, mortality = 57, infant = 2, alcohol = 3 ) # create dataframe of new point
predict(M1, newdata = new) 

# MODEL FOR HDB RESALE FLATS
resale = read.csv("hdbresale_reg.csv")
price = resale$resale_price
area = resale$floor_area_sqm
lm(price~area)$coef # coefficients of the model

# RSE
#Calculating RSE:
sqrt(sum((y - M1$fitted)^2)/(length(y) - 2))
summary(M) # Take the Residual standard error

# R^2
TSS = var(y)*(length(y) -1) # or
TSS = sum((y- mean (y)) ^2)
RSS =sum((y- M1$fitted )^2)
R2 = 1 - RSS/TSS; R2

# get r squared value
# multiple R-squared is the R square value 
summary(M1)$r.squared

# MULTIPLE LINEAR MODEL
set.seed(250) # randomises consistently for reproducibility
x1 = rnorm(100) # generating values from normal disturbution
x2 = rnorm(100) 
y = 1 + 2*x1 -5*x2+ rnorm(100)
lm(y~x1+x2)

# How to present fitted model
# y = −15257.5 + 77.99x + 30569.1I(NW = 1).

M.2 = lm(y~x1+x2)
# 3D plot to illustrate the data points
# plot3d(x1 , x2 , y, xlab = "x1", ylab = "x2", zlab = "y", type = "s", size = 1.5 , col = "red")

coefs = coef(M.2) # get the coefficients of the model
a <- coefs[2]; a # coef of x1
b <- coefs[3]; b # coef of x2
c <- -1       # coef of y in the equation: ax1 + bx2 -y + d = 0.
d <- coefs[1] # intercept
# planes3d (a, b, c, d, alpha = 0.5) # the plane is added to the plot3d above.


# MLR for HDB RESALE FLATS
resale = read.csv("hdbresale_reg.csv")
years_left = 2022 - resale$lease_commence_date # working a function on the column of data
price = resale$resale_price
area = resale$floor_area_sqm
M1 = lm(price ~ area)
summary(M1)
M2 = lm(price ~ area + years_left)
summary(M2) # on top p value is to see if that variable is significant or not (see number of stars) below p value is to see if overall model is significant

```
###### KNN classifier
```{r}
# Answering predict based on k value questions ***
# What is our prediction with K = 1? Why?
# Answer: Green. When K = 1, the one nearest point is Green (5th point, with distance of 1.41).
# Hence, for the test point, we classify it to the category that is the same as the category of the nearest point.

# What is our prediction with K = 3? Why?
# Answer: When K = 3, the three nearest points are the 5th (Green), 6th (Red) and 2nd (Red). 
# Hence, the test point will be classified as Red.

# K small or large?
# If the Bayes decision boundary (the gold standard decision boundary) in this problem is highly non-linear,
# then would we expect the best value for K to be large or small? Why?
# Answer: A small value for K, since it translates to a more flexible classification method.



# STOCK MARKET EXAMPLE
market = read.csv("Smarket.csv")
summary(market[,2:10]) #summary of data excluding response

#  PREPARING DATA TO FORM MODEL AND TO TEST MODEL
# to separate the data above to two parts: 
# one part is used to train the model
# another part is to test the model.
# We'll select the rows that belong to the years before 2005 to train model 
# index of the rows before year 2005 is in the vector "train":

train =(market$Year < 2005) # index of the rows in the years from 2001 to 2004 for training set

train.data = market[train,]
test.data  = market[!train,] # take the year 2005 as test set, the rest of rows in "market" is for testing model:

dim(train.data)
dim(test.data)

library(class)

# form a SET OF FEATURES for the training; and for testing: # choosing those columns and creating new data set
train.x = train.data[,c("Lag1","Lag2","Lag3","Lag4","Lag5")]
test.x = test.data[,c("Lag1","Lag2","Lag3","Lag4","Lag5")]
# form the RESPONSE for the training; and for testing:
train.y = train.data[,c("Direction")]
test.y = test.data[,c("Direction")]

#  FORMING MODEL = FORMING THE CLASSIFIER
set.seed(1)
knn.pred = knn(train.x,test.x,train.y,k=1)  # KNN with k = 1 # model comes up with fitted values of y for test.x # can vary k
# THE CLASSIFER FORMED BY KNN WAS CREATED, named knn.pred

# TO CHECK HOW GOOD THE CLASSIFER ABOVE IS, WE MAY USE ACCURACY:
confusion.matrix=table(knn.pred, test.y); confusion.matrix # does a comparison of actual and fitted
sum(diag(confusion.matrix))/sum(confusion.matrix) # 0.515873 # To calculate accuracy but not correct always make sure the down and up/ tp tn fp fn is in this order
# (55+75)/252 ~ 51.59% of the observations are correctly predicted

# to calculate precision of the classifier
set.seed (5)
knn.pred = knn(train.X,test.X,train.Y,k=1) # KNN with k = 1
confusion.matrix=table(test.Y,knn.pred)
confusion.matrix # Yes is in the second column/row
precision = confusion.matrix[2,2]/sum(confusion.matrix[,2])
precision

# N-FOLD CROSS VALIDATION 

#  A small example on dividing whole data set into n folds
n_folds=3
Number_of_datapoints=12 # sample size
index=rep(1:n_folds,length.out = Number_of_datapoints);index # assigns 1 2 3 12 times 
s = sample(index); s # sample randomises 1 2 3 
# sample randomises the order
table(s) 
# dataset of 12 points is devided into 3 folds randomly, each fold has 4 points.
# the 4 points for each of 3 folds are selected from the dataset following s. For example,
# s = 3 1 1 3 2 2 2 2 1 3 1 3
# then, the first data point belongs to 3rd fold. The next 2 points belong to 1st fold, etc.


# 5-fold Cross-Validation for KNN with k=1, 5, 10, etc. for the data set Smarket.csv
X=market[,c("Lag1","Lag2","Lag3","Lag4","Lag5")] # columns of explanatory features
Y=market[,c("Direction")] # response
dim(market) # 1250 data points/observations

n_folds=20
folds_j <- sample(rep(1:n_folds, length.out = dim(market)[1] )) 
table(folds_j)

err=numeric(n_folds) # create empty vector of 0s that is the length of n folds 
acc=numeric(n_folds)

for (j in 1:n_folds) {
	test_j <- which(folds_j == j) # get the index of the points that will be in the test set
	pred <- knn(train=X[ -test_j, ], test=X[test_j, ], cl=Y[-test_j ], k=1) # KNN with k = 1, 5, 10, etc # pred is the fitted value
  # -test_j means all data not in testj
	# test_j means all data in testj
	
	err[j]=mean(Y[test_j] != pred) # err j is the mean error of all the data rows
	acc[j]=mean(Y[test_j] == pred) # acc j is the mean accuracy of all the data rows
      # this acc[j] = sum(diag(confusion.matrix))/sum(confusion.matrix), where confusion.matrix=table(Y[test_j],pred)
}
err
acc
error=mean(err); error # mean of all the folds' errors
accur=mean(acc); accur # mean of all the folds' erors

# doing knn for k from 1 to 100
K = 100 # can try KNN with k = 1,2,...K.
accuracy=numeric(K) # to store the average accuracy of each k.
acc=numeric(n_folds) # to store the accuracy for each iteration of n-fold CV

for (i in 1:K){
  for (j in 1:n_folds) {
    test_j <- which(folds_j == j) # get the index of the points that will be in the test set
    pred <- knn(train=X[ -test_j, ], test=X[test_j, ], cl=Y[-test_j ], k=i) # KNN with k = 1, 5, 10, etc
    
    acc[j]=mean(Y[test_j] == pred) 
    # this acc[j] = sum(diag(confusion.matrix))/sum(confusion.matrix), where confusion.matrix=table(Y[test_j],pred)
  }
  accuracy[i] = mean(acc)
}

# to find three biggest accuracy of k
max(accuracy)
sort(accuracy)[98:100] # the three largest accuracy
index = which(accuracy == max(accuracy)) ; index # give index which is also the value of k.
plot(x=1:100, accuracy, xlab = "K")
abline(v = index, col = "red", ) # drawing a vertical straight line at the point which in this case is max

# another way to calculate accuracy
accuracy = sum(diag(confusion.matrix))/sum(confusion.matrix); accuracy

# if doing nested loop takes too long do both loops separately
K = 10
accuracy = numeric(K)
random <- 5
test_nej <- which(folds_negative == random)
test_pos <- which(folds_positive == random)
train_nej = negative_data[ -test_nej, ]
train_pos = positive_data[ -test_pos, ]
test_nej = negative_data[test_nej, ]
test_pos = positive_data[test_pos, ]
train_knn = rbind(train_nej, train_pos)
test_knn = rbind(test_nej, test_pos)

# We do the n folds loop outside of the K loop so as to ensrue the program runs in a reasonable amount of time
for (i in 1:K) {
  pred <- knn(train = train_knn[, 2:14], test = test_knn[, 2:14], cl = train_knn[,1], k = i)
  accuracy[i]= mean(test_knn[,1] == pred)
}
which(accuracy == max(accuracy))

```
###### Decision tree
```{r}
#install.packages("rpart")
#install.packages("rpart.plot")
library("rpart")
library("rpart.plot")
bankdata <- read.csv("bank-sample.csv")
fit <- rpart(subscribed ~job + marital + education+default + 
               housing + loan + contact+poutcome,
             method="class",
             data=bankdata,
             control=rpart.control(minsplit=1),
             parms=list(split='information')
             #or use "gini" for split
)
# which are the important features? ***
# It seems the sepal length and sepal width are not important in the classification 
# while the petal length and petal width are more important.


# there is another argument in rpart.control, that is cp.
# smaller values of cp correspond to decision trees of larger sizes, 
# and hence more complex decision surfaces.

# method = "anova", "poisson", "class" or "exp"
# If response is a survival object, then method = "exp" is assumed, 
# if response has 2 columns then method = "poisson" is assumed, 
# if response is a factor then method = "class" is assumed, 
# otherwise method = "anova" is assumed
# minslpit = 1: a stem is created when data have at least one observation in that stem
# split = 'information' or 'gini'

# To plot the fitted tree:
rpart.plot(fit, type=4, extra=2, varlen=0, faclen=0, clip.right.labs=FALSE)
rpart.plot(fit, type=4, extra=2)# can try with extra = 4 to see the difference #proportion instead of fraction
rpart.plot(fit, type=3, extra=2)# can try with extra = 4 to see the difference
rpart.plot(fit, type=3, extra=2, varlen=0, faclen=0, clip.right.labs=FALSE)
# only the bottom node will show
# for implementation specifics refer to notes

#varlen 4: variable name only 4 letters
#farlen 4: factor name only 4 letters
#varlen = length of variable's name,varlen = 0 means full name
#faclen = length of category's name, faclen = 0 means full name
#clip.right.labs: TRUE means: don't print the name of variable for the right stem
# You can try with varlen = 4 to see the difference compared to varlen = 0.
# name of variable only on left branch not right one if right.labs = TRUE

length(bankdata$poutcome)
table(bankdata$poutcome)

# ENTROPY PLOT 
p=seq(0,1,0.01)
D=-(p*log2(p)+(1-p)*log2(1-p))
plot(p,D,ylab="D", xlab="P(Y=1)", type="l")


# Calculating conditional entropy when 'poutcome' is splitted 
# as x1 = failure, other, unknown and x2 = success
x1=which(bankdata$poutcome!="success") # index of the rows where poutcome = x1
length(x1) # 1942 rows that the value of poutcome = x1.
x2=which(bankdata$poutcome=="success") # index of the rows where poutcome = x2
length(x2) # 58 rows that the value of poutcome = x2 = success
table(bankdata$subscribed[x1]) 
# counting how many "yes" and how many "no" for Subscribed among those with poutcome = x1
# among 1942 customers with poutcome = x1, 179 subscribed (179 yes), and 1763 no.
table(bankdata$subscribed[x2]) 
# counting how many "yes" and how many "no" for Subscribed among those with poutcome = x2
# among 58 customers with poutcome = x2, 32 subscribed (32 yes), and 26 no.

# Calculating conditional entropy when 'poutcome' is splitted 
# as x1 = success, other, unknown and x2 = failure
x1=which(bankdata$poutcome!="failure")
x2=which(bankdata$poutcome=="failure")
table(bankdata$subscribed[x1])
table(bankdata$subscribed[x2])

#  PLAYING GOLF EXAMPLE
library("rpart") # load libraries
library("rpart.plot")
play_decision <- read.table("DTdata.csv",header=TRUE,sep=",")
head(play_decision)

fit <- rpart(Play ~ Outlook + Temperature + Humidity + Wind,
             method="class",
             data=play_decision,
             control=rpart.control(minsplit=1),
             parms=list(split='information'))

rpart.plot(fit, type=4, extra=2)
newdata <- data.frame(Outlook="rainy", Temperature="mild",
                      Humidity="high", Wind=FALSE)
newdata
predict(fit,newdata=newdata,type="prob")
predict(fit,newdata=newdata,type="class")


# another way to predict with decision tree

predict(fit, newdata <- data.frame(Outlook="rainy", Temperature="mild",
                                   Humidity="high", Wind=FALSE), type = 'class') # get yes or no
predict(fit, newdata <- data.frame(Outlook="rainy", Temperature="mild",
                                   Humidity="high", Wind=FALSE), type = 'prob') # get probability

# If unsure
??rpart.plot

# Carrying 5 fold cv on data where each train and test set has equal obersvations of 3 diff type of flowers
library(rpart)
set.seed(555)
iris <- read.csv("iris.csv")
head(iris)
attach(iris)
table(species)

# N-Cross Validation for decision tree
# making sure equal number of the three responses in each fold and there fore eual in test and train

n_folds=5
folds_setosa <- sample(rep(1:n_folds, length.out = dim(iris)[1]/3 )) 
folds_virginica <- sample(rep(1:n_folds, length.out = dim(iris)[1]/3 )) 
folds_versicolor <- sample(rep(1:n_folds, length.out = dim(iris)[1]/3 )) 

table(folds_setosa)
table(folds_virginica)
table(folds_versicolor)

setosa_data = iris[1:50,]
virginica_data = iris[51:100,]
versicolor_data = iris[101:150,]

acc = numeric(n_folds)

for (j in 1:n_folds) {
  test_set <- which(folds_setosa == j)
  test_virg <- which(folds_virginica == j)
  test_vers <- which(folds_versicolor == j)
  
  train_setosa = setosa_data[ -test_set, ]
  train_virginica = virginica_data[ -test_virg, ]
  train_versicolor = versicolor_data[ -test_vers, ]
  
  test_setosa = setosa_data[test_set, ]
  test_virginica = virginica_data[test_virg, ]
  test_versicolor = versicolor_data[test_vers, ]
  
  train = rbind(train_setosa, train_virginica, train_versicolor)
  test = rbind(test_setosa, test_virginica, test_versicolor)
  
  fit.iris <- rpart(species ~ ., 
  #when using dot be careful that you dont include response inside if your "species" is a derived response variable
  #from the exact response variable
                    method = "class", data =train, control = rpart.control( minsplit =1),
                    parms = list( split ='gini'))
  
  pred = predict(fit.iris, newdata = test[,1:4], type = 'class')
  confusion.matrix = table(pred, test[,5])
  acc[j] = sum(diag(confusion.matrix))/sum(confusion.matrix)
  
}

acc
mean(acc)


#calculating best cp for decision tree
#tut 7 qn 3

library(rpart) 
library(rpart.plot)

banktrain <- read.csv("bank-sample.csv", header=TRUE)
dim(banktrain)

# total records in dataset
n=dim(banktrain)[1]; n

# drop a few columns to simplify the tree
drops<-c("age", "balance", "day", "campaign", 
         "pdays", "previous", "month", "duration")
banktrain <- banktrain [,!(names(banktrain) %in% drops)]
head(banktrain)

length(which(banktrain[,9] =="yes")) 
# indexes of all customers that equal that observtaion
# 211 out of 2000 customers subscribed.

# We'll randomly split data into 10 sets of (about) equal size
# regardless of percentage of "yes" in each set.

n_folds=10
folds_j <- sample(rep(1:n_folds, length.out = n))
# this is to randomly sample the indexes of subsets for the observation
# table(folds_j)

cp=10^(-5:5); length(cp) # take not of cp
misC=rep(0,length(cp)) # a vector to record the rate of mis-classification for each cp

# doing n cv for each cp
for(i in 1:length(cp)){
  misclass=0
  for (j in 1:n_folds) {
    test <- which(folds_j == j)
    train = banktrain[-c(test),]
    fit <- rpart(subscribed ~ job + marital + 
                   education+default + housing + 
                   loan + contact+poutcome, 
                 method="class", 
                 data=train,
                 control=rpart.control(cp=cp[i]),
                 parms=list(split='information'))
    
    new.data=data.frame(banktrain[test,c(1:8)]) # check if response there if not there remove it before inputting test or train data
    ##predict label for test data based on fitted tree
    pred=predict(fit,new.data,type='class')
    misclass = misclass + sum(pred!=banktrain[test,9])
  }
  misC[i]=misclass/n # total misclass for each n fold added and averaged
}

plot(-log(cp,base=10),misC,type='b') # plot misclassifictaion rate against cp

#'p': This is the default. It creates a scatterplot with points.
# 'l': It creates a line plot.
# 'b': It creates a plot with both points and lines connecting them.
# 'c': This is similar to 'b,' but without lines connecting the points.
# 'o': This is also similar to 'b,' but the points are overplotted on top of the lines.

## determine the best cp in terms of
## misclassification rate


best.cp =cp[which(misC == min(misC))] ; best.cp
# 0.01
# this is the value of cp that gives the lowest mis-classification rate

## Fit decision tree with that smallest cp
fit <- rpart(subscribed ~ job + marital + education+default + housing + loan + contact+poutcome, 
             method="class", 
             data=banktrain,
             control=rpart.control(cp=best.cp),
             parms=list(split='information'))

# to get the tree plotted:
rpart.plot(fit, type=4, extra=2, clip.right.labs=FALSE, varlen=0)#, faclen=3)

# ROC for Decision Trees:
titanic <- read.csv("Titanic.csv")
attach(titanic)

sur = ifelse(Survived == 'Yes', 1, 0) 
sur = as.factor(sur) 
titanic$sur = sur ; head(titanic)

M2<- rpart(sur ~ Class + Sex + Age, 
           method ="class",
           data = titanic,
           control = rpart.control(minsplit = 1),
           parms = list(split ='information'))

pred.M2 = predict(M2, titanic[,1:3], type='class') # no need to just take yes cuz its already a vector 
pred.M2= as.numeric(paste(pred.M2)) # changing format to numeric as pred is in classes

library(ROCR)
pred_dt = prediction(pred.M2, titanic$sur)
roc_dt = performance(pred_dt, measure="tpr", x.measure="fpr")
plot(roc_dt, add = TRUE) # add = True to add graoh only if axis is same (range of values on axes are same)

legend("bottomright", c("Naive Bayes","Decision Trees"),col=c("red","black"), lty=1)


auc2 = performance(pred_dt , measure ="auc")
auc2@y.values[[1]] # 0.683162

# WHEN WE FORM THE TREE USING SURVIVED (YES/NO) AND GET THE PROBABILITIES
# INSTEAD OF GETTING THE CLASS FOR OUTCOME, THEN:

# ROC for decision tree

M2<- rpart(Survived ~ Class + Sex + Age, 
           method ="class",
           data = titanic,
           control = rpart.control(minsplit = 1),
           parms = list(split ='information'))


rpart.plot(M2 , type =4, extra =2, clip.right.labs = FALSE , varlen =0, faclen =0)


# by probabilities

pred.M2 = predict(M2, newdata = titanic[,1:3], type = 'prob') # will get probbalities yes and no
pred.M2 = predict(M2, titanic[,1:3], type='prob')
score2 = pred.M2[,2] # here you have to take out the probability for yes since its not a vector in the necessary format

#prediction function only takes in numerical
pred_dt = prediction(score2, titanic$Survived)
roc_dt = performance(pred_dt, measure="tpr", x.measure="fpr")
plot(roc_dt)

legend("bottomright", c("Naive Bayes","Decision Trees"),col=c("red","black"), lty=1)


auc2 = performance(pred_dt , measure ="auc")
auc2@y.values[[1]] 
# 0.7262628
```
###### Naive bayers classifier
```{r}
# EXAMPLE 1: CLASSIFYING FRUITS
fruit.dat= read.csv("fruit.csv")
#Long/Sweet/Yellow: 1 = Yes, 0 = No
fruit.dat<- data.frame(lapply(fruit.dat, as.factor))
# lapplY to as.factor column by column
# as.factor to declare numeric factor as a categorical factor; changing type of example
# can as.factor one by one 
# for example can fruit.dat[,1] <- as.factor(fruit.dat[,1]) can do this manually for each column

head(fruit.dat)
attach(fruit.dat)

table(Long)
table(Sweet)
table(Yellow)

#Install package 'e1071' first
#install.packages("e1071")
library(e1071)

model <- naiveBayes(Fruit ~ Long+Yellow+Sweet,fruit.dat)

newdata <- data.frame(Long=1,Sweet=1, Yellow=0) # if for example Class = 2nd must use "2nd" and not 2nd
newdata <- data.frame(lapply(newdata, as.factor)) # as.factor here to create it
results <- predict (model,newdata,"raw") # probability of each response
results
results <- predict (model,newdata,"class") # default setting # Yes or No
results

# EXAMPLE 2: EMPLOYEE & ONSITE EDUCALTIONAL PROGRAM
sample <- read.table("sample1.csv",header=TRUE,sep=",")
# Enrolls = RESPONSE with 2 categories

# PART 1: MANUAL FORMING NAIVE BAYES CLASSIFIER
traindata <- as.data.frame(sample[1:14,])
testdata <- as.data.frame(sample[15,]) # in this instance no response variable for this row in the first place so no need to remove response variable later

# get the probability of each categories of the response, Compute the probabilities
tprior <- table(traindata$Enrolls);tprior
tprior <- tprior/sum(tprior); tprior # gives data in terms of probability, quite intuitive

# Get P(X = xi|Y = yj): row-wise proportion for each feature
# in table() first one is y second one is x
ageCounts <- table(traindata[,c("Enrolls", "Age")]);ageCounts # Get P(X = xi|Y = yj): row-wise proportion for feature AGE
ageCounts <- ageCounts/rowSums(ageCounts); ageCounts
incomeCounts <- table(traindata[,c("Enrolls", "Income")]) # Get P(X = xi|Y = yj): row-wise proportion for feature INCOME
incomeCounts <- incomeCounts/rowSums(incomeCounts);incomeCounts
jsCounts <- table(traindata[,c("Enrolls", "JobSatisfaction")]) # Get P(X = xi|Y = yj): row-wise proportion for feature JOBSATISFACTION
jsCounts <- jsCounts/rowSums(jsCounts);jsCounts
desireCounts <- table(traindata[,c("Enrolls", "Desire")]) # Get P(X = xi|Y = yj): row-wise proportion for feature DESIRE
desireCounts <- table(traindata[,c("Enrolls", "Desire")])/rowSums(desireCounts);desireCounts

#Applying the formular
# Proportion that point 15 will be "Yes" for the outcome is proportional to:
prob_yes <-
ageCounts["Yes",testdata[,c("Age")]]*
incomeCounts["Yes",testdata[,c("Income")]]*
jsCounts["Yes",testdata[,c("JobSatisfaction")]]*
desireCounts["Yes",testdata[,c("Desire")]]*
tprior["Yes"]

# Proportion that point 15 will be "No" for the outcome is proportional to:
prob_no <-
ageCounts["No",testdata[,c("Age")]]*
incomeCounts["No",testdata[,c("Income")]]*
jsCounts["No",testdata[,c("JobSatisfaction")]]*
desireCounts["No",testdata[,c("Desire")]]*
tprior["No"]

#MAKING DECISION:
prob_yes/prob_no #4.115226. Hence the 15th observation should be classified as YES. # dont need divide right can just do more or less than right


# PART 2: USE PACKAGE e1071 FORMING NAIVE BAYES CLASSIFIER
# install.packages("e1071s")
library(e1071)

# testdata  and train data already defined above but rewriting here for simplicity
traindata <- as.data.frame(sample[1:14,])
testdata <- as.data.frame(sample[15,]) # the response is not inside so no need to remove response variable

model <- naiveBayes(Enrolls ~ Age+Income+JobSatisfaction+Desire, traindata) #, laplace=0) # can have data = traindata as well
#laplace helps add a bias in case the probability of any calculation is 0 
results <- predict(model,testdata,"raw"); results
# raw - we want probabilities plotted and given to us
# class - we wont get detailed probability but final outcome
results[2]/results[1] # 4.115226

results <- predict(model,testdata,"class"); results # if you want yes or no

results <- predict(model,traindata[,1:4],"class"); results
cbind(results, traindata[,5] )
data.frame(results, traindata[,5]) 
```
###### ROC and AUC
```{r}
banktrain <- read.csv("bank-sample.csv", header=TRUE)
dim(banktrain)
head(banktrain)

# drop a few columns to simplify the tree
drops<-c("age", "balance", "day", "campaign", 
         "pdays", "previous", "month", "duration")
banktrain <- banktrain [,!(names(banktrain) %in% drops)]
head(banktrain)

# TESTING DATA SET
banktest <- read.csv("bank-sample-test.csv")
banktest <- banktest[,!( names ( banktest ) %in% drops )]

library(e1071)

# build the naive Bayes classifier
nb_model <- naiveBayes( subscribed ~., data = banktrain)

# perform on the test set BUT we need to remove the response column first
head(banktest);
ncol(banktest) # number of columns = 11. Response = 11th column.

nb_prediction <- predict(nb_model, newdata = banktest[,-ncol(banktest)], type ='raw') # remove last column
# if you use class instead of raw easier to compare later
# this is the predicted response for the test set
nb_prediction
cbind(nb_prediction, banktest[,ncol(banktest)])


# PLOT ROC CURVE FOR THE NAIVE BAYES CLASSIFIER ABOVE:
#install.packages("ROCR") 
# https://cran.r-project.org/web/packages/ROCR/ROCR.pdf

library(ROCR)
score <- nb_prediction[, c("yes")] # score is the conditional prob from Naive Bayes classifier for each test point
actual_class <- banktest$subscribed == 'yes' # actual response is 0 or 1

pred <- prediction(score , actual_class) # this is to "format" the input so that we can use the function in ROCR to get TPR and FPR
# repackage
perf <- performance(pred , "tpr", "fpr")
# calac diff tor and fpr values based on diff treshhold value

plot (perf, lwd =2) # lwd is to specify how thick the curve is
abline (a=0, b=1, col ="blue", lty =3)


# COMPUTE AUC FOR NAIVE BAYES CLASSIFIER:
auc <- performance(pred , "auc")@y.values[[1]] #to unlist #auc <- unlist(slot (auc , "y.values"))
auc
# auc is used to compare between Naive Bayes methd with other 
# methods such as linear model, logistic model, DT, etc. 
# the one with larger auc value is better.

# VISUALIZE ON HOW THE THRESHOLD CHANGES WILL CHANGE TPR AND FPR:
threshold <- round (as.numeric(unlist(perf@alpha.values)) ,4) # convert to numeric form
fpr <- round(as.numeric(unlist(perf@x.values)) ,4) #round to four decimal places
tpr <- round(as.numeric(unlist(perf@y.values)) ,4)
#see what are the chosen treshold values
# storing for and tpr values in a vector

# adjust margins and plot TPR and FPR
par(mar = c(5 ,5 ,2 ,5))
# mar = a numerical vector of the form c(bottom, left, top, right) = c(5,4,4,2)
# http://127.0.0.1:14187/library/graphics/html/par.html

plot(threshold ,tpr , xlab ="Threshold", xlim =c(0 ,1) ,
     ylab = "True positive rate ", type ="l", col = "blue")
par( new ="True") # treshhold vs tpr
plot(threshold ,fpr , xlab ="", ylab ="", axes =F, xlim =c(0 ,1) , type ="l", col = "red" )
axis(side =4) # to create an axis at the 4th side
# treshhold vs fpr
mtext(side =4, line =3, "False positive rate")
text(0.4 ,0.05 , "FPR")
text(0.6 ,0.35 , "TPR")

cbind(threshold,fpr,tpr)

#Manually calculating the probability of survivng or not
#classCounts, genderCounts, and ageCounts are conditional probbailities
#tpior is table(response)


# CONCLUDE: DECISION TREE IS BETTER THAN NAIVE BAYES
```

###### Logistic curve
```{r}
z = seq ( -10 ,10 ,0.1);
logistic = function (z) {exp(z)/(1+ exp(z))}

plot(z, logistic(z), xlab ="x", ylab ="p", lty =1, type ='l')


#  DATA SET ON CUSTOMER CHURN

churn = read.csv("churn.csv")
head(churn)

churn$Churned = as.factor(churn$Churned)
churn$Married = as.factor(churn$Married)
churn= churn[,-1] #Remove ID column

attach(churn)

table(Churned)
prop.table(table(Churned))

# LOGISTIC REGRESSION
# LOGISTIC MODEL

sur = ifelse(Survived == 'Yes', 1, 0)# response must be of 0 and 1 to fit the model
sur = as.factor(sur)
titanic$sur = sur ; head(titanic)
# Remb to do if not already in ones and zeros


M1<- glm( Churned ~., data =churn,family = binomial)
# ~ means depending on 
# . replaces all columns
summary(M1)

# FEMALE IS REFERENCE. MALE IS INDICATED BY INDICATOR since male can be seen in summary
# coefficient is estimated = -2.4201. 
# It means, given the same condition on the class and age,
# when comparing to a female, the LOG-ODDS of survival for a male is less than by 2.42.
# It means, the ODDS of survival of a male passenger will be less than that of a female by
# e^2.42 = 11.25 TIMES.


M2<- glm( Churned ~ Age + Married + Churned_contacts,
          data =churn,family = binomial(link ="logit"))
#Link = logit selected by default
summary(M2)

M3<- glm( Churned ~Age + Churned_contacts,
          data =churn,family = binomial(link ="logit"))
summary(M3)

predict(M3, newdata = data.frame(Age = 50, Churned_contacts = 5), type = 'response')
# type = 'response' means we want to get the Pr(Y = 1).


# ROC CURVE FOR LOGISTIC MODEL

library(ROCR)
prob = predict(M3, type ="response")
# dont need to specify data (new data = ...) cuz R will automatically predict the full data when you dont spcify

# above is to predict probability Pr(Y = 1) for each point in the training data set, using M3
# type = c("link", "response", "terms"). 
# http://127.0.0.1:14187/library/stats/html/predict.glm.html

pred = prediction(prob , Churned )
roc = performance(pred , "tpr", "fpr") # extract tpr and fpr
auc = performance(pred , measure ="auc")
auc@y.values[[1]] # area under the curve
plot(roc , col = "red", main = paste(" Area under the curve :", round(auc@y.values[[1]] ,4))) # rounding auc value to 4 dec places


# ROC for Logistic Regresison:(Another way)
# pred = predict(M2, type="response") # type = response to get the probability of survived
# pred_log = prediction(pred, titanic$Survived)
# roc_log = performance(pred_log, measure="tpr", x.measure="fpr")
# plot(roc_log, col = "red")

# auc1 = performance(pred_log , measure ="auc")
# auc1@y.values[[1]] # 0.7597259



# HOW TPR, FPR CHANGE WHEN THRESHOLD CHANGES:

# extract the alpha(threshold), FPR , and TPR values from roc
# treshold is alpha is here
alpha <- as.numeric(unlist(roc@alpha.values))
length(alpha) #328 couple values of fpr tpr used hereto create curve
fpr <- round(as.numeric(unlist(roc@x.values)) ,4)
tpr <- round(as.numeric(unlist(roc@y.values)) ,4)

x = cbind(alpha, tpr, fpr)
x

# adjust margins and plot TPR and FPR
par(mar = c(5 ,5 ,2 ,5))

plot(alpha ,tpr , xlab ="Threshold", xlim =c(0 ,1) ,
     ylab = "True positive rate ", type ="l", col = "blue")
par( new ="True")
plot(alpha ,fpr , xlab ="", ylab ="", axes =F, xlim =c(0 ,1) , type ="l", col = "red" )
axis( side =4) # to create an axis at the 4th side
mtext(side =4, line =3, "False positive rate")
text(0.18 ,0.18 , "FPR")
text(0.58 ,0.58 , "TPR")

# there are some metrics that can help to choose a threshold: G-mean; Youden’s J statistic; etc
```

###### K Means
```{r}
# HDB FLATS GROUPING
hdb=read.csv("hdbresale_cluster.csv")
head(hdb)
dim(hdb)

table(hdb$flat_type)

set.seed(1)


plot(x=hdb$floor_area_sqm, y=hdb$amenities,
     xlab="Floor area in sqm", ylab="Number of amenities", col="red") # plot one variable against another

kout <- kmeans(hdb[,c("floor_area_sqm","amenities")],centers=2)

plot(hdb$floor_area_sqm, 
     hdb$amenities, 
     col=kout$cluster)


kout$cluster # A vector of integers (from 1:k) indicating the cluster to which each point is allocated.

kout$centers # A matrix of cluster centres.

kout$size # The number of points in each cluster.

kout$withinss # Vector of SS_k, one value per cluster

kout$tot.withinss # Total within-cluster sum of squares = WSS


# PLOT TO SEE HOW WSS CHANGES WHEN K CHANGES

K = 10 # WE'LL TRY WITH k = 1, ...10.

wss <- numeric(K)

for (k in 1:K) { 
  wss[k] <- sum(kmeans(hdb[,c("floor_area_sqm","amenities")],centers=k)$withinss )
}

plot(1:K, wss, col = "red", type="b", xlab="Number of Clusters",  ylab="Within Sum of Squares")



# GRADE GROUPING

set.seed(1)
grade = read.csv("grades_km_input.csv")
head(grade)

attach(grade)

# VISUALIZE DATA SET BY FEATURES:
plot(grade[,2:4]) # plot a matrix
# PROPOSE: MIGHT BE 3 OR 4 GROUPS


kout <- kmeans(grade[,c("English","Math","Science")],centers=3)



plot(English, Science, col=kout$cluster)
plot(English, Math, col=kout$cluster)
plot(Math, Science, col=kout$cluster)

kout$withinss

# PLOT WSS vs K TO PICK OPTIMAL K:

K = 15 
wss <- numeric(K)

for (k in 1:K) { 
  wss[k] <- sum(kmeans(grade[,c("English","Math","Science")], centers=k)$withinss)
}


plot(1:K, wss, col = "blue", type="b", xlab="Number of Clusters",  ylab="Within Sum of Squares")

# comments:

# WSS is greatly reduced when $k$ increases from 1 to 2. 
# Another substantial reduction in WSS occurs at $k = 3$.

# However, the improvement in WSS is fairly linear for $k > 3$.
# Therefore, the $k$-means analysis will be conducted for $k = 3$.

# The process of identifying the appropriate value of k is
# referred to as finding the ``elbow'' of the WSS curve

# Calculate manually the eucleadin distance
x1 = c(1, 1.5, 3, 3.5, 4.5)
x2 = c(1,2,4,5,5)

plot(x1, x2, pch = 20, col = "blue")

text(1.1,1.1,"A") # labelling points on plot
text(1.6, 2.2, 'B')
text(3.1, 4.1, 'C')
text(3.63, 5, 'D')
text(4.35, 5, 'E')

# Adding the starting centroids 
points(2,2, pch = 2, col = 'red') # adding points to plot
text(2.2, 2.1, 'C-P') # adding text to plot
points(4,4, pch = 10, col = 'darkgreen')
text(4,3.8, 'C-Q')

# Adding the new centroids after the first iteration:
points(1.25, 1.5, col = 'red', pch = 2)
text(1.35, 1.4, 'C-P-new')
points(11/3, 14/3, col = 'darkgreen', pch = 10)
text(11/3, 4.5, 'C-Q-new')

# woring the k function on created values
data = data.frame(x1, x2)
data
kout = kmeans(data, centers = 2)
kout$withinss
kout$tot.withinss
```

######  Association rules 
```{r}
#install.packages('arules')
#install.packages('arulesViz')

# documentation of package 'arules'
# https://cran.r-project.org/web/packages/arules/arules.pdf

library('arules')
library('arulesViz')

data(Groceries)

?Groceries
# three parts itemsets, xxx ,datasets

summary(Groceries) # How does it look in the data set. # summary of all 3 parts

# this link below is helpful to understand this special data
# https://www.jdatalab.com/data_science_and_data_mining/2018/10/10/association-rule-transactions-class.html#:~:text=The%20Transactions%20Class,-The%20arules%20package&text=The%20Groceries%20data%20set%20contains,to%20read%20the%20Groceries%20data.

inspect(head(Groceries)) # the first 6 transactions # itemsets

Groceries@itemInfo[1:10,] # item
Groceries@data[,100:110] # data # row and column opposite

# the items for first 5 transactions:
apply(Groceries@data[,1:5], 2,
      function(r) paste(Groceries@itemInfo[r,"labels"], collapse=", ")) #function r is to convert dot and line to readable data

# the items for 100th to 105-th transactions:
apply(Groceries@data[,100:105], 2,
      function(r) paste(Groceries@itemInfo[r,"labels"], collapse=", "))

#  GETTING THE FREQUENT 1-ITEMSETS:

itemsets.1 <- apriori(Groceries, parameter=list(minlen=1, maxlen=1,
                                                support=0.02, target="frequent itemsets")) # if support of itemset more than 0.02 considered frequent

summary(itemsets.1)

# minlen = 1: frequent itemset has at least 1 item
# maxlen = 1: frequent itemset has max = 1 item
# set both 'minlen = 1' and 'maxlen = 1' means we want frequent itemset that has only 1 item.


# list the most 10 frequent 1-itemsets:
inspect(head(sort(itemsets.1, by = "support"), 10))

# list all the 59 frequent 1-itemsets:
inspect(sort(itemsets.1, by ="support"))


#  GETTING THE FREQUENT 2-ITEMSETS:

itemsets.2 <- apriori(Groceries, parameter=list(minlen=2, maxlen=2,
                                                support=0.02, target="frequent itemsets"))

summary(itemsets.2)

# list all the frequent 2-itemsets:
inspect(sort(itemsets.2, by ="support"))

# list of most 10 frequent 2-itemsets:
inspect(head(sort(itemsets.2, by = "support"), 10))


#  GETTING THE FREQUENT 3-ITEMSETS:


itemsets.3 <- apriori(Groceries, parameter=list(minlen=3, maxlen=3,
                                                support=0.02, target="frequent itemsets"))

summary(itemsets.3)
inspect(sort(itemsets.3, by ="support"))

# only TWO frequent itemsets that meets the minimum support of 0.02.

#  GETTING THE FREQUENT 3-ITEMSETS:

itemsets.4 <- apriori(Groceries, parameter=list(minlen=4, maxlen=4,
                                                support=0.02, target="frequent itemsets"))

summary(itemsets.4)

inspect(sort(itemsets.4, by ="support")) # nothing

# no 4-itemset satisfies the minimum support of 0.02. 
# If we lower down the minimum support to 0.007 then....

itemsets.4 <- apriori(Groceries, parameter=list(minlen=4, maxlen=4,
                                                support=0.007, target="frequent itemsets"))

summary(itemsets.4)
inspect(sort(itemsets.4, by ="support"))

# there are three frequent 4-itemsets if the minimum support is 0.007.


## # if the parameter maxlen is not specified, then....

itemsets<- apriori( Groceries , parameter = list( minlen=1,
                                                  support =0.02 , target ="frequent itemsets"))

summary( itemsets )
# this summarizes that: there are 59 frequent 1-itemsets; 
# 61 frequent 2-itemsets; and 2 frequent 3-itemsets

inspect(sort( itemsets , by ="support")) 
# this will rank the itemsets by their support, regardless of itemsets with 1 item or 2 items.
# row 17: {other vegetables, whole milk}  with support = 0.07483477

##  GETTING THE RULES instead of  FREQUENT ITEMSETS

rules <- apriori(Groceries, parameter=list(support=0.001,
                                           confidence=0.6, target = "rules")) #plotting itesmest with min suport = 0.001 and min confidence = 0.6

plot(rules) # scatter plot of all 2918 rules

# Scatter plot with custom measures and can add limiting the plot to the 100 with the 
# largest value for for the shading measure. 
plot(rules, measure = c("support", "confidence"), shading = "lift", col = "black")#, limit = 100) #can remove the )# #Whats the diff bw this and prev code - jst colour

#confidence, support and lift are like criteria for rules just like accuracy and others in other contexts

# more information about plot() under 'arules':
# http://127.0.0.1:23659/library/arulesViz/html/plot.html




# PLOT SOME TOP RULES FOR VISUALZATION:

# the top 3 rules sorted by LIFT:
inspect(head(sort(rules, by="lift"), 3)) #Sort by lift # Why not just plot lift?

# the top 5 rules sorted by LIFT
inspect(head(sort(rules, by="lift"), 5))
highLiftRules <- head(sort(rules, by="lift"), 5)

# plot the top 5 rules above for visualzation:
plot(highLiftRules, method="graph") # this is simple and a bit difficult to see

# more parameters added, plot looks better:
plot(highLiftRules, method = "graph", engine = "igraph",
     edgeCol = "blue", alpha = 1)
# alpha = c(0,1)
# the size of the node is sorted by the support.
# the darkness of the color represents the change in lift


plot(highLiftRules, method = "graph", engine = "igraph",
     nodeCol = "red", edgeCol = "blue", alpha = 1)
# this will fix the color be "red" for all lift values, 
# only the size of the node is sorted by the support.



#some common choices for 'method':
# matrix, mosaic, doubledecker, graph, paracoord, scatterplot, grouped matrix, two-key plot, matrix3D
















```
#### DATA IMPORT AND EXPORT
- read_excel(): Reads Excel files (.xls and .xlsx) into R.
- fromJSON(): Imports JSON data into R objects.
- write.csv(): Exports data to a CSV file.
- read.csv(): Reads CSV files into R.
```{r}
# install.packages("readxl")
library(readxl)
qn2_1 <- read_excel("tourist.xlsx", skip = 9) # skip the first nine lines of code # .xls and .xslx files, part of readxl package 
qn2_2 <- read_excel("tourist.xlsx", range = "A10:G26") # select specific region # Personally better
head(qn2_1)
# read_xlsx only for .xlsx files, and part of readr or readxl package|

removed_columns <- c("Total International Visitor Arrivals (Number)", "15 Days & Over (Number)", "Average Length Of Stay (Day)") # remove rows from the data
qn2_2 <- subset(qn2_1, !`Data Series` %in% removed_columns) # subset(): Used for filtering rows and columns from a dataset based on conditions.

# removed_columns <- c("Total International Visitor Arrivals (Number)", "15 Days & Over (Number)", "Average Length Of Stay (Day)")
# # Remove columns from data
# qn2_2 <- qn2_1[, !(colnames(qn2_1) %in% removed_columns)]

```




#### DYPLR PACKAGE 

- dplyr is a grammar of data manipulation, providing a set of functions that help us solve the most common data manipulation challenges

- work with input and output data frames

- Packages within the tidyverse include ggplot2 for visualization, dplyr for data manipulation, tidyr for data tidying, readr for data import, purrr for functional programming, tibble for modern data frames, and more.

- The tidyverse package itself is a meta-package that installs and loads these core packages.

###### Loading package
```{R}
#install.packages("tidyverse")
library(tidyverse) # Load package
```

###### Data exploration
- head(): Displays the first few rows of a dataset.
- glimpse(): Provides a compact overview of the dataset structure.
- class(): Returns the class of an object (e.g., data.frame, tibble).
- data(): Lists available datasets or loads built-in datasets.
```{r}
data(starwars)
head(starwars)
glimpse(starwars) # More information than head
class(starwars) # not dataframe but tibble (modern version of large datasets)

?starwars # In order to see the documentation
```

###### Key functions
- #IMPT
- %>% is pipe operator in dplyr to chain operations
- Do not need to use starwars$sex can just use sex, applies for all functions
- original dataframe not changed
- Base R for filter: starwars[(skin_color == "blue" & (sex == "female" | sex == "male")),]
- Base R for arrange: df1[order(mass), ] or df1[order(-mass), ] for descending
- Base R for filter subset function in order to filter part of the dataset df_summer <- subset(df, df$Month %in% c(5,6)) works similarly to filter
- | > pipe operator in base r

###### Data selection and subsetting
- subset(): Selects rows and columns based on specified conditions.
- filter() function subsets the rows in a data frame by testing against a conditional/logical statement
- select() to zoom in on a particular set of variables/ select specific columns
- count(): Counts unique combinations of variables.
- distinct(): Removes duplicate rows or identifies unique values.

###### Data manipulation
- mutate() function adds new columns of data, thus “mutating” the dimensions of the original data set/ transform columns
- arrange() function changes the order of observations in a data frame/ Orders rows by one or more columns
- group_by() groups data for grouped operations
- group_by() has no effect on the select() function, filter() and mutate() functions work within the group, arrange() function ignores groupings by default. We can turn it on by .by_group = TRUE,
- ungroup() removes the grouping structure 
- rename(): Renames columns.
- across(): Applies the same function(s) to multiple columns/ set of columns.
- slice_* functions: Extract specific rows (e.g., slice_max, slice_min).
- slice_head() and slice_tail() select the first or last rows
- slice_min() and slice_max() select rows with the smallest or
largest values of a variable
- summarize(), or summarise(), function creates individual summary statistics from large data sets, only the columns you explicitly define in the summarize() function are retained

###### Missing Data Handling
- na.omit(): Removes rows with missing values, or rows with na values


- unnest(): Expands list-columns into multiple rows.

###### Measures of data
- Measures of center: mean(), median()
- Measures of spread: sd(), var(), IQR()
- Measures of range: min(), quantile(), max()
- Measures of positions: first(x), nth(x, 2), last(x)
- Measures of count: n(), n_distinct()

```{r}
df <- filter(starwars, skin_color == "blue" & (sex == "female" | sex == "male")) # (Dataset, condition) 
df_2 <- filter(starwars, skin_color == "blue", sex %in% c("female", "male")) # %in% operator matches conditions provided in a vector constructed with c(). Filter out irrelevant columns

select(starwars, hair_color, birth_year) # To select columns located between hair_color and eye_color (inclusive)
select(starwars, -(hair_color:eye_color)) # To select columns except those located between hair_color and eye_color (inclusive) #IMPT
df_3 <- select(starwars, ends_with("color")) # creating new dataset with selected colunns
df_4 <- select(starwars, starts_with("hair"))
df_5 <- select(starwars, contains("_"))
df_6 <- select(starwars, num_range("x", 1:3)) # matches columns x1, x2, and x3

df1 <- select(starwars, name, height, mass, species) 
df2 <- mutate(df1, height_m = height/100, .before = name) # creates a new column to the right of the original data frame # before argument to add the variable before name column

arrange(df1, mass) # arrange according to mass, can add more columns to right to break ties according to new columns
arrange(df1, desc(mass)) # arrange a column in descending order

summarize(starwars, height = mean(height, na.rm = TRUE)) # compute the average height for Star Wars characters, overriding height column, na.rm argument will ignore missing values

df3 <- tibble(name = c("Alex" ,"Jay" ,"Cam" ,"Lily" ,"Haley" ,"Joe"), 
              status = c("full time" ,"part time" ,"part time" ,"part time" ,"full time" ,"unknown"), 
              age = c(19, 49, 34, NA, NA, 10)) # creating new tibble

# Replace all NA values with 0 in a tibble or data frame
# df <- df %>% mutate(across(everything(), ~replace(., is.na(.), 0)))


df4 <- group_by(df3, status) # R will read it as a grouped object now, can group by multiple variables
summarize(df3, mean_age = mean(age, na.rm = TRUE))
summarize(df4, mean_age = mean(age, na.rm = TRUE)) # applying summarise on grouped dataset
filter(df4, age >= mean(age, na.rm = TRUE)) # Within each group keep rows with value larger or equal to the group mean

df4_1 <- df4 %>% # remb to use pipe operator so that column actually gets added to the dataset
  mutate(sum_age = cumsum(replace_na(age, 0))) # Replace NAs with 0, cumsum - cumulative sum add in order of values
df4_2 <- df4 %>% # remb to use pipe operator so that column actually gets added to the dataset
  mutate(m_age = sum(replace_na(age, 0))) # sum order of row dont matter, replace na values with 0

arrange(df4, desc(age), .by_group = TRUE) # arrange in descending order in terms of group, by default ignores grouping unless turned on by using .by_group, arrange within the group
df5 <- ungroup(df4) # use ungroup() at the end of a series of grouped operations

df3 %>% group_by(status) %>% arrange(desc(age), .by_group = TRUE) %>% ungroup() # arranges the data within the groups

starwars_by_sex <- group_by(starwars, sex) 
summarize(starwars_by_sex, mean_mass = mean(mass, na.rm = TRUE))

starwars %>%
  filter(!is.na(sex)) %>% # Remove na entries
  group_by(sex) %>% # Group by sex
  summarize(mean_mass = mean(mass, na.rm = TRUE)) # summarise mean by sex
```

###### JSON
- rbind(): Combines rows from multiple datasets into one.
```{r}
library(jsonlite)
base_url <- "https://data.gov.sg/api/action/datastore_search" 
resource_id <- "?resource_id=d_1a88e269bf1d629b93fb5cafa189f9fb" 
url <- paste0(base_url, resource_id)
results_json <- fromJSON(url)
results_json[["result"]][["records"]]
results_json[["result"]][["_links"]]
results_json[["result"]][["_links"]][["next"]]
results <- results_json[["result"]][["records"]]
total_records <- results_json[["result"]][["total"]]
while(nrow(results) < total_records) {
  next_url <- paste0("https://data.gov.sg",
                     results_json[["result"]][["_links"]][["next"]])
  results_json <- fromJSON(next_url)
  results <- rbind(results, results_json[["result"]][["records"]])
}
write.csv(results, "../data/imda.csv", row.names = FALSE)

imda <- read.csv("../data/imda.csv",
                 header = TRUE, stringsAsFactors = TRUE)

young <- imda %>% 
  filter(age == "20-29", year == 2015) %>% mutate(ever_used = as.numeric(ever_used)) %>% 
  arrange(desc(ever_used))
head(young, 2)
```

```{r}
starwars %>% 
  summarize(shortest = first(name, order_by = height)) # first - give first observation, order by - arrange data frame internally by some variable, Find the shortest character

starwars %>% summarize(across(height:mass, min, na.rm = TRUE)) # applying same function minimum on every column from height to mass, finding mean across columns 
starwars %>% summarize(across(where(is.numeric), mean, na.rm = TRUE)) # Look at positions that are numeric, and find mean of all columns that are numeric
starwars %>% mutate(across(where(is.character), tolower)) %>% head() # To make uppercase words lower case, edit all columns according to a condition
starwars %>% na.omit() # Remove all missing values
starwars %>% filter(!is.na(gender))# Remove missing values in "gender"
starwars %>% mutate(across(where(is.character), na_if, "none")) # replacing none with NA across all columns
starwars %>% mutate(across(where(is.character), na_if, "none")) %>% na.omit() # remove missing values
starwars %>% distinct() # Remove duplicated rows
starwars %>% distinct(sex) # finding all unique categories of sex
starwars %>% count(sex) # Count occurrences of unique sex
starwars %>% count(sex, species) # Count occurrences of unique combinations of sex and species # similar to group by
starwars %>% count(sex, species, sort = TRUE) # In descending order
starwars %>% rename(character_name = name) %>% head(3) # Rename "name" as "character_name"

# rename(id = record, weight_kg = stweight) # to rename multiple ones at the same time

starwars %>%
  group_by(gender) %>% 
  slice_max(mass, n = 1) %>% # obtaining heaviest for each gender, obtaining column highest
  relocate(gender, mass) # change column positions


starwars_films <- starwars %>% # Previous films information stored in films
  select(name, films) %>% 
  unnest(films) # expand a list column of a tibble into rows and columns
head(starwars_films)
```

```{r}
# install.packages("nycflights13")
library(nycflights13)
data(flights) 
head(flights)

flights %>% filter(month == 9, day == 6) # Select all flights on September 6 

flights_JFK <- flights %>% 
  filter(origin == "JFK") %>% 
  select(year:day, dest, ends_with("delay"), distance, dep_time) # selecting several columns: those between year and day, dest and those that end with delay

flights_JFK <- flights_JFK %>% 
  mutate(hour = dep_time %/% 100, # divisor
         minute = dep_time %% 100, .after = day) %>% head(3) # Compute hour and minute from dep_time # remainder, insert column after day

flights_status <- flights %>% 
  mutate(arr_status = ifelse(is.na(arr_delay), "cancelled", # If na assign cancelled
                             ifelse(arr_delay > 0, "late", "on time"))) # three conditions, simple conditions use ifelse
flights_status %>% count(arr_status)

flights %>% # Assign catgeroical values based on numeric values #IMPT
  mutate(arr_status = 
           case_when(is.na(arr_delay) ~ "cancelled", 
                     arr_delay <= 0 ~ "on time",
                     arr_delay > 0 ~ "late")) %>%  # case when better for more conditions, much clearer and more robust code, can set True ~ "" (Like remaining that was not caught by previous two conditions)
count(arr_status) # Count each status

flights_JFK %>% group_by(month) %>% # Get group summaries, Monthly mean departure delay, and sort them in decreasing order
  summarize(mean_dep_delay = mean(dep_delay, na.rm = TRUE)) %>%
  arrange(desc(mean_dep_delay))

flights %>% filter(origin == "JFK") %>% 
  count(dest) %>% # Keep observations above a certain value #IMPT
  filter(n > 5000)
```


#### Tidy data
- Each variable is a column
- Each observation is a row
- Each cell is a single value

###### Data transformation(Tidy data)

- gather() gathers columns into a new pair of variables: Identify the set of columns that supposed to represent values but is represented as variables, key is now the label for the old represented variables and value is now the label for the old values under these old represented variables [makes data long and narrow], [makes a data set long and narrow]
- spread() helps spread trhe two different variable observations that are stored as values in the old variable. Column that has the multiple variable names is labelled as key and the column with the multiple variable values is labelled as the value, [makes data wide and short]
- pivot_longer(): Converts wide data to long format.
- pivot_wider(): Converts long data to wide format.
- seperate() pulls apart one column into multiple columns by splittig wherever a seperator character appears, Splits a single column into multiple columns.
- unite() combines multiple colunmns into one using a seperator character

```{r}
library(tidyverse)
data(table1)
data(table2)

# Extract the number of TB cases per country per year
table1 %>% select(country, year, cases) # optional to focus on certain columns
table2 %>% filter(type == "cases") %>% select(country, year, count)

# Doing analysis with tidy data
table1 %>% mutate(rate = cases/(population/10000))

table2 %>% mutate(insh = lag(count)) -> table_insh  # move down the values
table2 %>% mutate(insh = lead(count)) -> table_mush # move up the values

# Doing analysis with untidy data
# lag value in count column ordered by type "moving value down"
# select relevant columns and rows after doing neccesary calculations
table2 %>% arrange(country, year, type) %>% 
  group_by(country, year) %>% 
  mutate(temp = lag(count, order_by = type)) %>% # Essentially, it shifts the count column by one position for each group
  mutate(rate = temp/(count/10000)) %>% 
  filter(type == "population") %>% 
  select(country, year, rate)

# Tidying up the untidy data table4a
table4a %>% gather(`1999`:`2000`, key = "year", value = "cases")

data(table2)
table2 %>% spread(key = type, value = count) # key value classes become new column name, spreads out key and put in new columns, value is column to take the values from
# makes table 2 into 1

# same as gather()
data(table4a)
table4.1a <- table4a %>% pivot_longer(`1999`:`2000`, # Same as making narrower
                         names_to = "year", values_to = "cases")

# Same as spread
data(table2)
table2.1 <- table2 %>% pivot_wider(names_from = type, values_from = count)

# If got multiple values in column, in this case seperate column into two columns
table3 %>% separate(rate, 
                    into = c("cases", "population"), convert = TRUE, sep = "/", remove = FALSE) # sep is optional, remove = false is optional if u want keep original 

# unite() to rejoin these columns
table5 %>% unite(col = year, century:year, sep = "") # Be carfeul unite alone will remove other columns

data(billboard)
glimpse(billboard)
billboard %>% distinct(track) %>% count() # how many unique songs
billboard %>% distinct(artist, track) %>% count()
billboard %>% count(track, sort = TRUE) # two artists have released song with same name, sorted counts
# To tidy, week as new col, rank col take values
billboard %>%
pivot_longer(wk1:wk76, names_to = "week", values_to = "rank") %>% head(8) # More arguements
data(billboard)
billboard %>%
pivot_longer(wk1:wk76, names_to = "week", values_to = "rank", 
             values_drop_na = TRUE) %>% # removes na after pivoting to longer format, another way of selecting comments is to do !wk0 instead of wk1:wk76 or can do !(wk77:wk78) instead
mutate(week= as.numeric(str_remove(week, "wk"))) -> df 

df <- df %>% 
  mutate(date_rank = date.entered + 7 * (week - 1)) %>% 
  select(-date.entered) # then remove column (date entered) for compactness, negative select is to drop the variable

# The number of weeks a song track was in Top 100
df %>% count(artist, track, sort = TRUE)

# The number of weeks a song track occupied top 1
df %>% 
  filter(rank == 1) %>% 
  count(artist, track, sort = TRUE) %>% head() # Or slice max n = 10 cuz slice max dont required sorted values
# Top 10 artists that have made the longest reign in the Billboard Top 100
df1 <- df %>% count(artist, sort = TRUE) %>% slice_max(n, n = 10) 
par(mar = c(5, 6, 2, 2)) # Adjust margin of plot
barplot(df1$n, names.arg = df1$artist,
        horiz = TRUE, las = 1, # las equals to 1 fix text ,hori = true makes bar hori
        cex.names = 0.6, cex.axis = 0.6, border = NA,
        main = "The most popular artists in the year 2000")

```

```{r}
library(tidyverse)
library(nycflights13)
data(airlines)
data(weather)
weather %>% relocate(time_hour, .after = origin) %>% head() # Relocate time hour column right next to origin

# Checking the unqiueness of keys
planes %>% 
  count(tailnum) %>% filter(n > 1) # double check if primary key is unique, no duplicated tail numbers here, check this before connecting

planes %>% summarize(n = n_distinct(tailnum)) # also can do this instead

weather %>% 
  count(time_hour, origin) %>% filter(n > 1)

planes %>% filter(is.na(tailnum)) # check for missing values in the primary keys, check this before connecting

weather %>% filter(is.na(time_hour) | is.na(origin)) # check if have na in time hour or origin, left table, do the same for the right table, at least one of them needs to be unique

# To ease demonstration, let’s first create a narrower data frame
flights2 <- flights %>% 
  select(time_hour, origin, dest, tailnum, carrier)
head(flights2)

# left_join() starts with a data frame of interest, Preserves all records from the left-hand table, i.e., avoid unintentional data loss!, Easier to understand and predict the outcome of data joins.

flights2 %>% left_join(airlines, by = "carrier") # Joined by carrier column, repetitions present since many to one join, mutating join since xtra column added, flights will be on the left and carrier on the right

flights2 %>% 
  left_join(planes %>% select(tailnum, engines, seats), by = "tailnum") # Joining by tailnum, reuslt of mutating join, many to one join

# We intentionally remove tail num for that one. Therefore no exact match and got missing values, cuz tailnum cannot be matched
flights2 %>% 
  filter(tailnum == "N3ALAA") %>% 
  left_join(planes %>% select(tailnum, engines, seats), by = "tailnum")

# Demonstration

x <- tibble(key = c(1, 2, 3), 
            val_x = c("x1", "x2", "x3"))
y <- tibble(key = c(1, 2, 4), 
            val_y = c("y1", "y2", "y3"))

# Inner join only can see matched rows, intersection
x %>% inner_join(y, by = "key")

# Outer join got three tyeps: left_join(), right_join() and full_join()

x %>% left_join(y, by = "key") # Dont lose info on left join, A U ANB, use as default join
x %>% right_join(y, by = "key") # dont lose info on right side, B U ANB
x %>% full_join(y, by = "key") # Keep eveything, dont lose any info on either side, union

# If one table has duplicated keys, then the matching row will be duplicated as well

x <- tibble(key = c(1, 2, 3), 
            val_x = c("x1", "x2", "x3"))
y <- tibble(key = c(1, 2, 2), 
            val_y = c("y1", "y2", "y3"))
x %>% inner_join(y, by = "key")

# In most cases, you need to have unique keys for at least one of your tables. Check for this Avoid my to many join to avoid many duplicated rows. Can result in size explosion.

# If common variables have same name can use by arguement empty, sequence of order of varibales in by shoudkl eb same as that in dataset

# When names of variables are different, so you explicitly state
flights2 %>% left_join(airports, by = c("dest" = "faa"))

# semi_join(x, y): keeps all observations in x that have a match in y, intersection, If there are duplicated keys in x, all those rows will be kept. Fewer rows but same number of columns

top_dest <- flights %>% 
  count(dest, sort = TRUE) %>% slice_max(n, n = 10) # or can use head(10)
flights2 %>% semi_join(top_dest)

# anti_join(x, y): keeps all observations in x that do not have a match in y, A N B', keeps only the unmatched observations in x, find lost information, helpful to diagnose joint mismatches

flights %>% 
  anti_join(planes, by = "tailnum") %>% 
  count(tailnum, sort = TRUE) %>% pull(tailnum) -> temp # pull saves as a vector all mismatched tailnums

# to get all planes that cannot be matched with flights table

flights2 %>% filter(tailnum %in% temp) %>% 
  count(carrier, sort = TRUE)

# To do:
# 1. Identify the primary keys in each variable.
# ▶ Use count() in conjuncture with filter().
# 2. Check that none of the variables in the primary key are missing. If a value is missing, it cannot identify an observation.
# ▶ Use filter() with is.na().
# 3. Check that foreign keys match primary keys in another table.
# ▶ The best way to do this is an anti_join().

# Inequality join using numbers
sales <- tibble(sales_date = ymd("2024-09-01", "2024-09-03", "2024-09-14", "2024-09-17"))
promos <- tibble(promo_date = ymd("2024-09-09", "2024-09-14"), promo_price = c(179, 199)) # ymd makes it date time variable

sales %>% inner_join(promos, by = c("sales_date" = "promo_date"))

# Inequality join: Matching rows where sales_date occurred after promo_date.
sales %>% inner_join(promos, join_by(sales_date > promo_date)) # More than equals to

sales %>% inner_join(promos, join_by(closest(sales_date > promo_date))) -> musolla # closest matching rows

# Concatinate rows or columns

# if we have two or more data frames with the same columns, we can bind the rows efficiently with bind_rows()
sales_aug <- tibble(sales_date = ymd("2024-08-01", "2024-08-03")) 
bind_rows(sales_aug, sales) # Name must be the same

# bind the columns with bind_cols(). Rows are matched by position. So the data frames must have the same number of rows.
prices <- tibble(prices = c(209, 219, 219, 219)) 
bind_cols(sales, prices)

# Useful when u have quarterly or monthly update of data. New information all in same format so dn merge can concatinate
# Watch last 12 mins of fri lecture 6
```

```{r}
# Tidying

# qn2_3 <- read_xlsx("tourist.xlsx", range = "A10:G26") %>% 
#   slice(-c(1,12,16)) %>%  # Remove row 1,12,16
#   mutate(k = row_number(), .before = 1) %>% # Add k as row number as first columns
#   pivot_longer(`2022 Dec`:`2022 Jul`, names_to = "month", values_to = "arrivals") %>%
#   separate(month, into = c("year", "month"), convert = TRUE) %>% # seperate into month column e.g "2022 Dec" into year and month e.g "2022" and "Dec"
#   mutate(month = factor(month, levels = month.abb, ordered = TRUE)) %>% # converts the month column of a data frame into an ordered factor, using the three-letter month abbreviations ("Jan", "Feb", ..., "Dec") as its levels. By specifying ordered = TRUE, the code ensures that the months are treated in their natural calendar order rather than alphabetically. This is useful when you need to perform time-based comparisons or ensure that months are displayed chronologically in plots or analyses.
#   rename(duration = `Data Series`) %>% 
#   arrange(year, month, k) # arrange by year, then month then by k
# 
# qn2_2 <- qn2_1 %>% 
#   group_by(duration) %>% # group durations together
#   mutate(temp = lag(arrivals, order_by = month), rate = (arrivals - temp)* 100/temp) %>% # lag shift values
#   ungroup(duration) %>% 
#   select(-temp) %>% # remove temp column
#   filter(!is.na(rate)) # remove na values
# 
# qn2_2 %>% arrange(year, k) %>% head(4) # arrange by year then by k
# 
# 
# qn1_1 <- read_excel("imf-dm-export.xls") %>%
#   pivot_longer(2:51,names_to = "year", values_to = "inflation") %>%
#   filter(inflation != "no data") %>% # to remove rows with "no data" as the entry
#   mutate(year = as.numeric(year), inflation = as.numeric(inflation)) %>%
#   rename(country = "Inflation rate")

data(who2)
who2_tidy <- who2 %>% 
  pivot_longer(cols = !(country:year), 
               names_to = c("method", "sex", "age"), names_sep = "_", 
               values_to = "count", values_drop_na = TRUE)
head(who2_tidy) # to convert e.g "sp_m_014" to value then to convert into "sp" "m" and "014" values

continents <- read_excel("../data/UNSD — Methodology.xlsx")
glimpse(continents)

continents <- continents %>% 
  rename(region = 1, 
         country = 2, 
         iso = 3, 
         least_developed = 4, 
         land_locked = 5, 
         small_island = 6) %>% # renaming the columns by choosing them by order for example renamingh firts column as region and so on and so forth
  mutate(across(least_developed:small_island, 
                function(x) ifelse(is.na(x), 0, 1))) # recoding the values as 1 if have x and as 0 if have n.a/no entry
glimpse(continents)

who2_country <- who2_tidy %>% 
  group_by(country, year) %>% 
  summarize(total = sum(count, na.rm = TRUE)) %>% ungroup()

# after checking for unqiueness of keys on both sides, and removing na rows on both sides

who2_country %>% 
  left_join(continents, by = "country") # More important on the left

who2_country %>% 
  anti_join(continents, by = "country") %>% 
  count(country) # To find rows in who2_country that cannot be matched with continents- Possible reasons: name of country coded differently in continent table OR country does not exist in continent table

continents %>% 
  anti_join(who2_country, by = "country") %>% 
  count(country) # check for other way round: rows in continents that cannot be matched with who2_country

who2_continent <- who2_country %>% 
  mutate(country = ifelse(country== "Czech Republic", 
                          "Czechia", country)) %>% #rename as neccesary cuz the name in right table just differently coded from name in left table
  left_join(continents, by = "country") %>% 
  mutate(region = as.factor(region)) %>%
  # Remove unmatched rows 
  filter(!is.na(region)) # Remove remaining rows with no region data after the above matching. 
glimpse(who2_continent)
```

#### Common visualisations
```{r}
# target_month <- month.name[5:9] # converts numbers to months
# df$Month <- factor(df$Month, levels = 5:9, labels = target_month) # data, order, connecting label to data 
# introducing order to the plot
# boxplot(df$Temp ~ df$Month, main = "Monthly temperature in New York", xlab = "Month", ylab = "Temperature (degrees F)")


# month <- factor(c("Jan", "Feb", "March", "April", "May", "June", "July", "Aug", "Sep", 'Oct', "Nov", "Dec"), levels = c("Jan", "Feb", "March", "April", "May", "June", "July", "Aug", "Sep", 'Oct', "Nov", "Dec"))
# 
# Temp <- c(4, 4, 9.5, 16.5, 21.5, 24, 29, 28, 23.5, 20, 12,5)
# 
# plot(as.numeric(month), Temp, 
#      type ='b',
#      pch = 20,
#      xlab = "Month",
#      ylab = "Temperature (degrees c)",
#      main = "Monthly temperature in Shanghai 2012",
#      xaxt = 'n')
# 
# axis(1, at = 1:length(month), labels = levels(month))



# qn2_3 <- qn2_2[1:2]
# qn2_3$pct <- qn2_2$`2022 Dec` / sum(qn2_2$`2022 Dec`) # remb to use tilda `
# barplot(qn2_3$pct, names.arg = c(0:7, "8-10", "11-14", "15-29", "30-59", "60+"), las = 2,
#         main = "Tourists' Length of Stay, Dec 2022", xlab = "Number of days", 
#         ylab = "Percentage of Arrivals")



# plot(qn1_1$date, qn1_1$price, type = "l", lwd = 4, las = 1, xlab= "", ylab = "",
#      main = "Quarterly median house prices in the United States" )



# plot(qn1_3$date, qn1_3$price, type = "o", pch = 16, lwd = 2, las = 1,
#      main = "Quarterly median Housing prices (dollars), 2019 − 2024", xlab = "", ylab = "")

# qn1_4 <- qn1_2 %>% 
#   filter(country == country_of_interest, between(year, 2005, 2023)) # Filter countries and years
# 
# title = paste("Inflation in", country_of_interest, "(2005−2023)") # Title
# 
# mean <- qn1_4 %>% 
#   summarize(mean_inflation = mean(inflation)); mean



# 
# barplot(qn1_4$inflation, # Plottign y values
#         xlab = "", ylab = "Inflation Rate (%)", main = title,# if x lab no label just do "".
#         names.arg = qn1_4$year, col = "steelblue", border = NA, las = 2) # printing the x values here
# abline(h = mean, 
#        lty = "dashed", col = "lightgray") # Printing out the mean value line
# mtext(paste0("Average inflation rate: ", round(mean, 2), "%")) # Printing subtext at the top



# plot(qn2_1$date, qn2_1$Hospitalised, # Most important 
#      xlab = "", ylab = "Number of Cases", main = "Covid-19 Weekly Statistics in Singapore", # Second most important 
#      type = "l", lwd = 3, col = "steelblue", las = 1) # Least important 
# lines(qn2_1$date, qn2_1$ICU, # Most important
#       type = "l", lwd = 3, col = "black") # Least important 
# legend("topleft", legend = c("Hospitalisations", "ICU Admissions"), # Most important
#        lty = 1, lwd = 2, col = c("steelblue", "black")) # Least important

# Bar plot for categorical data
df1 <- who2_continent %>% 
  filter(year== 2000) %>% count(region, sort = TRUE)
barplot(df1$n, names.arg = df1$region, # region is the name on the axis and n is the height
        main = "Number of countries and regions represented in 2000", 
        xlab = "Continents", ylab = "Number of countries and regions", 
        border = "white", las = 1)

# Distribution of TB cases in Africa, Histogram more for continuous data
df2 <- who2_continent %>% filter(region== "Africa", year== 2000) 
hist(df2$total/1000, 
     main = "Distribution of total TB cases in Africa in 2000",  
     xlab = "Total cases (in thousands)", ylab = "Frequency", 
     border = "white", las = 1)

# Box plot for comparison, to compare continuous data across groups
df3 <- who2_continent %>% 
  filter(year >= 2000, 
         country %in% c("China, Hong Kong SAR", "Singapore"))
par(mar = c(5, 10, 2, 2))
boxplot(df3$total~ df3$country, horizontal = TRUE, 
        main = "TB cases across countries and regions", 
        xlab = "Total TB cases", ylab = "", las = 1)

# Line chart for TB cases across year
df4 <- who2_continent %>% filter(region== "Asia") %>% 
  group_by(year) %>% summarize(med = median(total)) %>% ungroup()
plot(df4$year, df4$med, 
     type = "o", lwd = 4, las = 1, # Type o refers to connected line chart
     main = "Median TB cases in Asia across years", 
     xlab = "Year", ylab = "Median TB cases")

# Prepare data
df5 <- who2_continent %>% 
  filter(region %in% c("Asia", "Europe")) %>% 
  group_by(year, region) %>% 
  summarize(med = median(total)) %>% 
  pivot_wider(names_from = region, values_from = med) %>% 
  ungroup()
glimpse(df5)

# Multiple lines
# Use different colours and different colours of lines, type of line is refered to by the lty arguement.
plot(df5$year, df5$Asia, type = "l", lwd = 4, las = 1, lty = 1, 
     main = "Median TB cases in Asia and Eurioe across years", 
     xlab = "Year", ylab = "Median TB cases")
lines(df5$year, df5$Europe, lwd = 4, lty = 2, col = "orange")
legend("topleft", legend = c("Asia", "Europe"), 
       lwd = 4, lty = c(1, 2), col = c("black", "orange"))
```

####  GGPLOT2

- Loaded in tidyverse
- High quality
- Dont need to use dollar sign just call the variables directly
- Impt components: base ggplot(), data, geom function, set of aesthetics mappings (position, size, shape, colour of point, type of line)
- Optional, scales, geoms, theme
- Add layers(+)
- Global setting overides local setting

One numeric variable: Density plot, histogram
One/Two categorical variables: Bar plot, tree map
Two numeric variable: Line plot, Scatter plot
Three variables: Scatter plot, Bubble plot, Area plot
One numeric, one categorical variable: Violin plot, Boxplot

- "install.packages("ggplot2")"

ggplot(data = <DATA>) + 
  <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>),
                  stat = <STAT>,
                  position = <POSITION>) +
<COORDINATE_FUNCTION> +
<FACET_FUNCTION>

```{r}
library(ggplot2)
library(tidyverse)
data(mpg) # available in baser
head(mpg, 2)
```
```{r}
ggplot() # Blank canvas
```
- ggplot(): The base function to initiate the plotting. It sets up the canvas, where you define data and aesthetics (aes).
- geom_point(): Used to create scatter plots by plotting points for each observation.
- geom_line(): Used to draw lines connecting data points in a time-series or trend plot.
- geom_boxplot(): Plots a boxplot to compare distributions of a continuous variable across categories.
- geom_bar(): Creates bar charts based on counts of a categorical variable.
- geom_col(): Similar to geom_bar(), but requires you to specify both the x and y variables for the heights of the bars.
- geom_histogram(): Plots a histogram to display the distribution of a continuous variable.
- geom_density(): Displays a smoothed version of a histogram, useful for visualizing the distribution of a variable.
- geom_smooth(): Adds a smooth curve to data, typically for trend analysis, with options for different smoothing methods (e.g., linear regression, loess).
- geom_polygon(): Used to create polygons (for mapping or regions) by connecting points.
- geom_abline(): Adds a reference line with a specified slope and intercept to a plot.
- geom_vline() / geom_hline(): Adds vertical or horizontal reference lines to a plot, often used for marking thresholds.
- geom_text(): Adds text labels to the plot at specified coordinates.
- geom_treemap(): For creating tree maps, used to visualize hierarchical data where both size and color are important.
- geom_bin2d(): Plots a 2D histogram to display the density of points in two continuous variables.
- geom_hex(): Similar to geom_bin2d(), but uses hexagons instead of squares to represent the binned data points.
- geom_ridges(): Used for creating ridgeline plots, typically to compare the density distributions of a variable across groups.
- geom_area(): Used to create stacked area charts to visualize cumulative data over time.
- geom_abline(): Adds a reference line (usually a linear regression line).
- scale_(): Functions like scale_color_, scale_fill_ control the coloring of elements in the plot based on the data.

Types of Visualizations:
Bar Plot (geom_bar() and geom_col()):
Used to display categorical data, either by counts (geom_bar()) or by specific values (geom_col()).
Stacked or grouped bar plots can be created by manipulating the position argument.
Line Chart (geom_line() and geom_point()):
Used for time-series or continuous data. Often combined with geom_smooth() to show trends.
Boxplot (geom_boxplot()):
Used to compare distributions across different groups, useful for showing outliers, medians, and spread.
Histogram (geom_histogram()):
Displays the distribution of a continuous variable. Can be modified with binwidth and fill to control appearance.
Density Plot (geom_density()):
A smoothed version of the histogram, showing the probability distribution of a continuous variable.
Scatter Plot (geom_point()):
Plots individual points to display relationships between two continuous variables. Can be customized with color, size, and alpha.
Treemap (geom_treemap()):
Used to visualize hierarchical data, mapping size and color to categories and their values.
Smooth Line (geom_smooth()):
Adds a smoothed line (e.g., linear regression or local regression) to a plot to visualize trends and relationships between variables.
Facet Wrapping (facet_wrap()):
Divides the plot into smaller subplots based on the values of a categorical variable, useful for comparing categories in a plot.
Maps (geom_polygon()):
Plots geographical data, using latitude and longitude to define polygons, useful for maps and spatial data visualization.
Aesthetics (aes):
Mapping variables to visual properties: In ggplot, you can map variables to visual aesthetics like color, size, shape, fill, and transparency using the aes() function.
Themes:
theme_minimal(): A theme with minimalistic design, removing background gridlines and other elements.
theme_void(): A theme with no axes, labels, or background.
theme_classic(): A theme with classic gridlines and axes.
theme_economist(): A predefined theme from the ggthemes package for a more publication-style appearance.
Common Operations:
labs(): To add titles, axis labels, and legends.
scale_*: To modify scales for continuous or discrete data, including colors, sizes, and other properties.
coord_*: Used to control the coordinate system, such as setting an orthographic map projection (coord_map()).
Special Use Cases:
Log Transformation (scale_x_log10(), scale_y_log10()):
Often used for variables with skewed distributions, allowing for a better visual comparison across large ranges of values.
Color Palettes (scale_fill_viridis_c(), scale_fill_distiller()):
Use colorblind-friendly color scales like viridis or YlGnBu for continuous variables.
Time Series (geom_line() and geom_point() with aes(x = year)):
Used for plotting trends over time, often combined with labels or smooth lines for clarity.
Geospatial (geom_sf()):
Used to plot spatial data with sf objects, often for mapping regions or countries.


###### Scatterplot
```{r}
# Break the line after the addition sign

ggplot(data = mpg) + # ggplot call, input data source
  geom_point(mapping = aes(x = displ, y = hwy)) # choose function for plot and aesthetics(map variables to axis) 

# Better implementation
ggplot(mpg, aes(x = displ, y = hwy)) + # Global default, easier, dont need add spcifically on every layer unless changed locally for each layer
  geom_point(aes(color = class)) # Local, legend added, colour shoudl be in local not global as vairble called "colour name" will be added to the dataset

# Same implementation, now colour in global wont change for each layer
ggplot(mpg, aes(x = displ, y = hwy, color = class)) + # Global default, BUT DONT CHOOSE COLOUR AS A COLOUR HERE
  geom_point()

ggplot(cars, aes(x = speed, y = dist)) + 
  geom_point(color = "red", alpha = 0.5, size = 3) # not discriminating by variables, alpha (transparency, good for overlapping points) or do jittering (best for those with many datapoints), tweak positon by a little bit (random variation)

ggplot(cars, aes(x = speed, y = dist)) + 
  geom_point(position = "jitter", color = "red", alpha = 0.5, size = 4)

ggplot(cars, aes(x = speed, y = dist, color = speed > 15)) + # plot colour based on values
  geom_point(size = 4) + 
  labs(title = "Relationship between Speed and Braking", 
       x = "Speed (mph)", y = "Braking distance (ft)", 
       color = "Speed > 15") + 
  theme_minimal()

```

###### Smooth line
- geom_smooth()
```{r}

ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + 
  geom_point(position = "jitter") + # Can flip order of geom_point() and geom_smooth() if you wanna specify order
  geom_smooth(method = "lm", formula = y~ x, se = FALSE) + # Adding smooth line on top of points, did not specify any aesthetic mapping so will use global settings, can specify method and fomular for smooth function # SE = fALSE if you dw the shaded region (no standard error)
  labs(x = "Engine Displacement (l)", y = "Highway Miles per Gallon") # Adding labels

# Fit higher order polynomial
ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + 
  geom_point(position = "jitter") + 
  geom_smooth(method = "lm", formula = y~ poly(x, 2)) + # Higher order of polynomial 
  labs(x = "Engine Displacement (l)", y = "Highway Miles per Gallon")

# fit local regression model just for surrounding points, locally weighted model
ggplot(mpg, aes(x = displ, y = hwy)) + 
  geom_point(position = "jitter") + 
  geom_smooth(method = "loess", formula = y~ x) + 
  labs(x = "Engine Displacement (l)", y = "Highway Miles per Gallon")

# Different smoothness with different spans (Larger span- mopre smooth)

# smooth diff line based on class (type of car)
ggplot(mpg, aes(x = displ, y = hwy, col = drv)) + #  specify diff colour for diff class of drv (global)
  geom_point(position = "jitter") + 
  geom_smooth(aes(linetype = drv), formula = y~ x, # specify diff type of line for diff class of drv
              method = "loess", 
              show.legend = FALSE) + # remove legend for geom smooth since same legend for legend for geom_point()
  labs(x = "Engine Displacement (l)", y = "Highway Miles per Gallon") + # Can add colour = "Drive type" here instead of doing the scale_color_discrete function
  scale_color_discrete(name = "Drive type", # colour - chaning legend box assoc with colour, # discrete - cuz discrete variable
                       labels = c("4-wheel","Front-wheel", "Rear-wheel")) + 
  theme(legend.position = "top") # Change posiiton of legend
```

###### Histograms
- Divide values into bions, no of observations - height
- Only x values required

- For reference lines
- geom_vline() for vertical lines
- geom_hline() for horizontal lines
- geom_abline() for straight lines defined by a slope or an intercept

- lty or linetype specifies the type of the line.
- lwd or linewidth controls the thickness of the line.

# install.packages(c("ggthemes", "ggrepel"))
```{r}
heights <- read.csv("../data/heights.csv", header = TRUE, stringsAsFactors = TRUE)

ggplot(data = heights, aes(x = earn/1000)) + 
  geom_histogram()

ggplot(data = heights, aes(x = earn/1000)) + 
  geom_histogram(binwidth = 10, boundary = 0, # Binwidth 10,000, Bins now begin at 0 now since money cannot be negative
                 color = "white", fill = "indianred4") + # White is color of border, fill is colour of interior
  labs(title = "Histogram of Earnings", 
       x = "Earnings Per Annum (in Thousands)", y = "Frequency") # Title and labels

# Plotting in terms of density and not observations
ggplot(data = heights, aes(x = earn/1000, y = after_stat(density))) + 
  geom_histogram(binwidth = 10, boundary = 0, fill = "indianred4") + 
  labs(title = "Histogram of Earnings", 
       x = "Earnings Per Annum (in Thousands)", y = "Density")

ggplot(data = heights, 
       aes(x = earn/1000, y = after_stat(density), fill = sex)) + # Map class of variable to fill, legend box will change accordingly
  geom_histogram(binwidth = 10, boundary = 0) + 
  labs(title = "Histogram of Earnings", 
       x = "Earnings Per Annum (in Thousands)", y = "Density")

ggplot(data = heights, 
       aes(x = earn/1000, y = after_stat(density), colour = sex)) + # Map class of variable to colour of border , legend box will change accordingly
  geom_histogram(binwidth = 10, boundary = 0) + 
  labs(title = "Histogram of Earnings", 
       x = "Earnings Per Annum (in Thousands)", y = "Density")

# Dont combine the bars
ggplot(data = heights, 
       aes(x = earn/1000, y = after_stat(density), fill = sex)) + 
  geom_histogram(binwidth = 10, boundary = 0, position = "dodge") + 
  labs(title = "Histogram of Earnings", 
       x = "Earnings Per Annum (in Thousands)", y = "Density")

# Plotting and comparing densities
ggplot(heights, aes(x = earn/1000, fill = sex)) + 
  geom_density(alpha = 0.2) + # alpha indicates some transparency so that can see the overlap
  labs(title = "Smooth Density Plots of Earnings", 
       x = "Earnings Per Annum (in Thousands)", y = "Density") + 
  scale_fill_discrete(name = "Gender", labels = c("Female", "Male")) # Chnage legend box to fill, and variable was discrete

# bw argument controls smoothness, by default bandwidth chosen as 1, too wide - over smoothing, masks important patterns and distributions, too low - under smoothing, too many bumps in data captured, too many details

# Suitable for plotting time series
library(readxl)
UN_data <- read_excel("../data/UNESCAP_population_2010_2015.xlsx", 
                      sheet = "Pop- women")
selected_countries <- c("Singapore", "Malaysia", 
                        "Thailand", "Indonesia")
pop1 <- UN_data %>% 
  pivot_longer(Y2010:Y2015, names_to = "year", values_to = "population" ) %>%
               mutate(year = as.integer(str_sub(year, 2, 5))) %>% 
                 filter(e_fname %in% selected_countries)

ggplot(data = pop1) +
  geom_line(aes(x = year, y = population, color = e_fname))# Color ensures that we plot by country, defining groups in data

# If we wish to group without colour, but no legend anymore now
ggplot(pop1, aes(x = year, y = population)) + 
  geom_point() + 
  geom_line(aes(group = e_fname))

# Labelling values
df_text <- filter(pop1, year== 2015) # ending year in dataset is 2015, do this if you wish to add label to end of the line
df_text

ggplot(data = pop1, aes(x = year, y = population, group = e_fname)) + 
  geom_line(linewidth = 1.5) + 
  geom_text(data = df_text, aes(label = e_fname), # labelling e_fname, specify new dataframe overide global, no need to specify x and y again cuz name didnt change from ones in
            hjust = "left", nudge_x = 0.1, size = 3.5) + # align left of label to data, posiitve value nudge x to move right, can also nudge y to go up or down
  # Use geom_text instead of geom label for background of the label
  xlim(2010, 2015.7) # adjust xlim as needed to ensure full label printed

# For reference lines
ggplot(data = heights, aes(x = earn/1000, y = after_stat(density))) + 
  geom_histogram(binwidth = 10, boundary = 0, fill = "indianred4") + 
  geom_vline(aes(xintercept = 100), linetype = 2, linewidth = 0.3) + 
  labs(title = "Histogram of Earnings", 
       x = "Earnings Per Annum (in Thousands)", y = "Density")
```

###### Case study
```{r}
murders <- read.csv("../data/murders.csv")

r <- murders %>% 
  summarize(rate = sum(total) / (sum(population/1e6))) %>% 
  pull(rate) # extract rate as a single number, get vector of length one (in this case) using pull
log10_r <- log10(r) # to get intercept of line
log10_r


library(ggthemes)
library(ggrepel) # Dont need change labels using nudge etc. unlike geom_text() even if change scales, not perfect but an improvement, still got issues sometimes so revert back to geom_text() if any issues

ggplot(murders, aes(x = population/1e6, y = total)) + 
  geom_abline(slope = 1, intercept = log10_r, linetype = 2, color = "darkgray") + # check order of plotting if line is above plots or vice versa in this case more in background
  geom_point(aes(col = region), size = 3) + 
  geom_text(aes(label = abb), nudge_x = 0.07) + 
  # Can replace with geom_text_repel(aes(label = abb), color = "black") + makes life easier for positioning of label
  scale_x_log10() + scale_y_log10() + # Scale the axes
  labs(title = "US Gun Murders in 2010", 
       x = "Population in millions (log scale)", 
       y = "Total number of murders (log scale)", color = "Region") +
  theme_economist() # Can choose the theme

ggsave("../My_notes/wk10_murders.png") # To save the plots
```

###### Barchart
- geom_col() makes the height of the bar represent values in the
data. (Need give x and y variables)
- geom_bar() counts the number of cases in each group and makes
the height proportional to counts. (Need only give x variable)

- install.packages("viridis")
- install.packages("gridExtra")
- install.packages("hexbin")
- install.packages("patchwork")
```{r}

ggplot(op_budget, aes(x = budget_cat, y = amount)) + 
  geom_col() + 
  labs(title = "Operating Budget", 
       x = "Amount (million dollars)", y = "Expenditure Category")

df_imda <- read.csv("../data/imda.csv", 
                    header = TRUE, stringsAsFactors = TRUE)
df_young <- filter(df_imda, age== "20-29", year== 2015) %>% 
  mutate(pct = as.numeric(ever_used))

ggplot(df_young, aes(x = media_activity, y = pct)) + 
  geom_col() + 
  coord_flip() + # Flip coordinate so that ore space to write the variables
  labs(title = "Percentage of Young Adults ...", 
       x = "Activity", y = "Percentage")

# To plot bars in descending order (Must do if factors are order eg Q1,Q2,Q3,Q4)
df_young <- df_young %>% 
  mutate(media_activity = reorder(media_activity, pct))

ggplot(df_young, aes(x = media_activity, y = pct)) + 
  geom_col() + 
  coord_flip() + 
  labs(title = "Percentage of Young Adults ...", 
       x = "Activity", y = "Percentage")

df_tv <- df_imda %>% 
  filter(media_activity %in% c("Watch MediaCorp TV", 
                               "Watch StarHub TV", 
                               "Watch Singtel TV")) %>% 
  mutate(pct = as.numeric(ever_used))

p1 <- ggplot(df_tv, aes(x = year, y = pct)) + 
  geom_col(aes(fill = media_activity), 
           position = "dodge") + # To seperate the bars
  facet_wrap(~ age) + # To split the charts by age
  labs(title = "Percentage of Young Adults ...", 
       x = "Activity", y = "Percentage", fill = "") + 
  theme(legend.position = "top")
p1

library(viridis)
p1 + scale_fill_viridis(option = "viridis", discrete = TRUE) # Use a color package, e.g., the viridis colorblind-friendly palette. Ask for discrete colour palette, can use continuous plattes as well

# 2D histogram
# To prevent overplotting if too many points opverlap, previously we used trnasparency or jitter to do it but hers a alternative
p1 <- ggplot(mpg, aes(x = displ, y = hwy)) + 
  geom_bin2d(binwidth = c(0.5, 5)) + 
  scale_fill_gradient(name = "Count", low = "white", high = "red") + # Map loght colour to low value and dark colour to high value
  labs(title = "2D Bins", 
       x = "Engine Displacement (l)", y = "Highway Miles per Gallon")+
theme(legend.position = "top")
p1

# Use hexagons instead of rectangles
p2 <- ggplot(mpg, aes(x = displ, y = hwy)) + 
  geom_hex(binwidth = c(0.5, 5)) + 
  scale_fill_gradient(name = "Count", low = "white", high = "red") + 
  labs(title = "Hexagonal Bins", 
       x = "Engine Displacement (l)", y = "Highway Miles per Gallon")+
theme(legend.position = "top")
p2

# To show multiple graphs together
library(gridExtra)
grid.arrange(p1, p2, nrow = 1) # Use n row for graphs above one or the other

# Alternative way to show multiple graphs together
library(patchwork)
p1 + p2
# p4 | (p1 / p2) can do this for p4 on the left nd p1 on top right and p2 at top right
```

###### Maps
- geom_polygon()
- need the latitude and longitude of the boundaries for different regions
# install.packages("maps")
# install.packages("mapproj")
# install.packages("sf") # For geospatial data
```{r}
library(maps)

# Load US states boundaries map
us_states <- map_data("state")
# Load murders data
murders <- read.csv("../data/murders.csv")

head(us_states)

# Visualise longtitude and latitude
ggplot(data = us_states, aes(x = long, y = lat)) + # Connect sequentially and not across anyhow
  geom_point(size = 0.25) 

ggplot(data = us_states, aes(x = long, y = lat, group = group)) + 
  geom_polygon()

ggplot(data = us_states, aes(x = long, y = lat, group = group)) + 
  geom_polygon(color = "white", fill = "lightblue")

ggplot(data = us_states, aes(x = long, y = lat, group = group, 
                             fill = region)) +
geom_polygon(color = "white", show.legend = FALSE) # Legend is not neccesary here as not informative and useless

# Merge data by state and region
murders <- murders %>% 
  mutate(state = tolower(state)) %>%   # Or capitalise region column, murders have more important info so left join
  left_join(us_states, by = c("state" = "region"))
head(murders)

# Change data
ggplot(data = murders, aes(x = long, y = lat, group = group)) + 
  geom_polygon(aes(fill = total),color = "white") + # Color is boundary, colour of fill is intensity but intensity is reverse to what we want

  
# Change data
ggplot(data = murders, aes(x = long, y = lat, group = group)) + 
  geom_polygon(aes(fill = total),color = "white") + 
  scale_fill_gradient(low = "lightblue", high = "steelblue") # Therefore we set the colour gradient

ggplot(data = murders) + 
  geom_polygon(aes(x = long, y = lat, group = group, 
                   fill = population/1000000), color = "white") + 
  scale_fill_continuous(name = "Population (millions)", 
                        low = "lightblue", high = "steelblue") + 
  theme_minimal() + 
  theme(legend.position = "bottom", 
        axis.title = element_blank(), axis.text = element_blank(), 
        axis.ticks = element_blank())

# World map
world <- map_data("world")
ggplot(data = world, aes(x = long, y = lat, group = group)) +
  geom_polygon(fill = "lightgray", color = "white") +
  theme_minimal() +
  theme(axis.title = element_blank(), axis.text = element_blank(),
        axis.ticks = element_blank()) -> world_map
world_map

# To look down from the north pole
world_map + 
  coord_map(projection = "orthographic")

# center it at a particular location (Singapore) include sg coordinates
world_map + 
  coord_map(projection = "orthographic", orientation = c(1, 103, 0))

# Highlight asean
asean <- map_data("world", 
                  region = c("Indonesia", "Laos", "Malaysia",
"Philippines", "Thailand", "Myanmar",
"Cambodia", "Vietnam", "Singapore"))
ggplot(data = world, aes(x = long, y = lat, group = group)) + 
  geom_polygon(fill = "lightgray", color = "white") + 
  geom_polygon(data = asean, fill = "indianred4") + 
  theme_minimal() + 
  theme(axis.title = element_blank(), axis.text = element_blank(), 
        axis.ticks = element_blank()) + 
  coord_map(projection = "orthographic", orientation = c(1, 103, 0))

# How to map different parts of Singapore
library(sf)
sg_map <- readRDS("../data/sg_masterplan2019.rds")
# class(sg_map)

# Need to use geom_sf instead cuz different kind of data
ggplot(data = sg_map) + 
  geom_sf(aes(geometry = geometry), 
          fill = "lightgray", color = "white")

# Play around with themes
# p5 <- p3 + theme_void()
# p6 <- p3 + theme_classic()
# p7 <- p3 + theme_minimal()
# p3 + p5 + p6 + p7 # using patchwork

```

###### Financial Time's visual vocab
- "install.packages("gapminder")"
- "install.packages("ggridges")"
- "install.packages("treemapify")"
- at most 5-6 lines in a graph if u want audience to detect differences
```{r}

library(gapminder)
library(tidyverse)
theme_set(theme_minimal()) # Can set this at start so do not need to keep repeating
data(gapminder)

### Change over timer
# Connected line chart is geom_point and geom_line together
gapminder %>% filter(country == "Singapore") %>% # Can use pipe operator with ggplot2
  ggplot(aes(x = year, y = lifeExp)) + 
  geom_line(size = 1) + 
  geom_point(size = 2) + 
  labs(title = "Life expectancy at birth in Singapore", 
       x = "", y = "Age (years)") 
# can add + ylim(0,85) to limit the line

gapminder %>% 
  filter(country %in% c("Singapore", "Japan")) %>% 
  ggplot(aes(x = year, y = lifeExp, color = country)) + # or can use 
  # group = country if u just want black but seperated like previouslky
  geom_line(size = 1) + 
  geom_point(size = 2) + 
  labs(title="Life expectancy at birth in Singapore and Japan", 
       x = "" , y = "Age (years)", color = "") + 
  theme(legend.position = "top")

# Slope chart (Show changes over time, only focus on beginning point and end point, dont care about changes in between)
df <- gapminder %>% 
  filter(country %in% c("Singapore", "Germany"), 
         year %in% c(1952, 2007)) 
ggplot(df, aes(x = year, y = gdpPercap, 
               group = country, color = country)) + 
  geom_line(size = 2, alpha = 0.8, show.legend = FALSE) + 
  geom_point(size = 5, alpha = 0.9) + 
  labs(title = "GDP per capita, 1952 - 2007", 
       x = "", y = "", color = "") + 
  theme(legend.position = "left")

# stacked area chart chart, line with the areas (use geom_area), stacked lines, dont show lines but stack areas
gapminder %>% 
  filter(country %in% c("France", "Germany", "Ireland")) %>% 
  ggplot(aes(x = year, y = gdpPercap, fill = country)) + 
  geom_area(color = "white", alpha = 0.4) + # colour of fill and background
  labs(title = "GDP per capita by country", 
       x = "", y = "GDP per capita ($)", fill = "") + 
  theme(legend.position = "top")

# Can use geom_col to do same thing
gapminder %>% filter(country== "Singapore") %>% 
  ggplot(aes(x = year, y = gdpPercap)) + 
  geom_col(alpha = 0.8) + 
  labs(title = "GDP per capita in Singapore", 
       x = "", y = "GDP per capita ($)") + 
  scale_x_continuous(breaks = seq(1952, 2007, 5))

### Correlation
# scatterplot to display the correlation between two continuous variables.
gapminder %>% filter(year== 2007) %>% 
  ggplot(aes(x = gdpPercap, y = lifeExp)) + 
  geom_point(alpha = 0.8, size = 2) + 
  scale_x_log10() + # To suppress higher values and make smaller values bigger basically changing scale
  labs(title = "Life expectancy and income, 2007", 
       x = "GDP per capita ($)", y = "Age (years)")

# Add colour to indicate continent info
gapminder %>% filter(year== 2007) %>% 
  ggplot(aes(x = gdpPercap, y = lifeExp)) + 
  geom_point(aes(size = pop, color = continent), alpha = 0.8) + # Map size of point to population value
  scale_x_log10() + 
  labs(title = "Life expectancy and income, 2007", 
       x = "GDP per capita ($)", y = "Age (years)") + 
  guides(size = FALSE) # We just need relative size so can remove the size legend but keep the colour legend

### Deviation
# diverging bar chart
gapminder %>% filter(year== 1952, continent== "Americas") %>% # In this case combine manipulation to data frame (preperation of data) with variable but can split up intp manipulation and visualisation seperately
  mutate(med = median(gdpPercap, na.rm = TRUE), 
         diff = gdpPercap- med, 
         type = ifelse(gdpPercap < med, "Below", "Above")) %>% 
  arrange(diff) %>%  
  mutate(country = factor(country, levels = country)) %>% # ordering countries in terms of difference, based on sorted data /levels
  ggplot(aes(x = country, y = diff, fill = type)) + 
  geom_col(width = 0.5, alpha = 0.8) + 
  labs(title = "GDP per capita, 1952", 
       x = "", y = "", fill = "") + 
  coord_flip()

# Alternative way to do diverging bar chart
df1 <- gapminder %>% filter(year== 1952, continent== "Americas") %>% 
  mutate(med = median(gdpPercap, na.rm = TRUE), 
         diff = gdpPercap- med, 
         type = ifelse(gdpPercap < med, "Below", "Above")) %>% 
  mutate(country = reorder(country, diff)) # Changing order of country using redorder

df1 <- ggplot(data = df, aes(x = country, y = diff, fill = type)) + 
  geom_col(width = 0.5, alpha = 0.8) + 
  labs(title = "GDP per capita, 1952",x = "", y = "", fill = "") + 
  coord_flip()

### Distirbution
# histogram displays the distribution of a continuous variable.
gapminder %>% filter(year== 2002) %>% 
  ggplot(aes(x = lifeExp)) + 
  geom_histogram(color = "white", alpha = 0.8) + 
  labs(title = "Distribution of life expectancy, 2002", 
       x = "Age (years)", y = "", fill = "")

# density plot is a smoothed version of a histogram
gapminder %>% filter(year== 2002) %>% 
  ggplot(aes(x = lifeExp, fill = continent)) + 
  geom_density(alpha = 0.5) + 
  labs(title = "Distribution of life expectancy, 2002", 
       x = "Age (years)", y = "", fill = "") +
theme(legend.position = "top") # Ocenia dont have meaningful density plot as only represented by two countries

# ridgeline plot is useful for comparing the density of a variable across groups (we dont wish to worry about the y values then can use this)
library(ggridges) # one of hundreds of extensions eg gganimate
gapminder %>% filter(year== 2002, continent != "Oceania") %>% 
  ggplot(aes(x = lifeExp, y = continent, fill = continent)) + 
  geom_density_ridges(alpha = 0.8) + 
  labs(title = "Distribution of life expectancy, 2002", 
       x = "Age (years)", y = "", fill = "") + 
  theme(legend.position = "none")


### Magnitude - size of variable or category
# geom_bar() creates bars to represent the frequency of observations.
# geom_col() creates bars to represent values in the data.

# bar chart
# Shortcut to visualise by group using geom_bar
# To add annotations
# geom_bar + geom_text()
gapminder %>% filter(year== 2007) %>% 
  ggplot(aes(x = continent)) + # Only x var required for geom_bar
  geom_bar(alpha = 0.8) + 
  geom_text(stat = "count", aes(label = after_stat(count)), 
            nudge_y = 2, size = 5) + # Can add colour argumenet
  labs(title = "Number of countries by continent", 
       x = "", y = "Number of countries")

# If you wish to geom_col instead (longer cause need prepare the data)
# No annotations here
gapminder %>% filter (year == 2007) %>% 
  count(continent) %>% 
  ggplot(aes(x = continent, y = n)) + 
  geom_col(alpha = 0.8)

# Ordered bar chart
# Sort by frequency/ count, only if categorical variable is unordered, eg no time order
gapminder %>% filter(year== 2007) %>% 
  ggplot(aes(x = fct_infreq(continent))) + # fct_infreq() reorders the factor levels based on frequency
  geom_bar(alpha = 0.8) + 
  geom_text(stat = "count", aes(label = after_stat(count)), 
            nudge_y = 2, size = 5) + 
  labs(title = "Number of countries by continent", 
       x = "", y = "Number of countries")

# Ordered bar chart
# Order by median value
gapminder %>% filter(year== 2007) %>% 
  group_by(continent) %>% 
  summarize(med = median(gdpPercap)) %>% 
  ggplot(aes(x = fct_reorder(continent, med), y = med)) + # If you want opposite order replace med by -med
  geom_col(alpha = 0.8) + 
  labs(title = "Median GDP per capita by continent, 2007",  
       x = "", y = "Median GDP per capita ($)")

# Stacked bar chart
options(scipen = 999) 
gapminder %>% filter(year >= 1992) %>% 
  group_by(year, continent) %>% 
  summarize(totalpop = sum(pop)) %>% 
  ggplot(aes(x = year, y = totalpop, fill = continent)) + 
  geom_col(alpha = 0.8) + 
  labs(title = "Total population by continent", 
       x = "", y = "Population") + 
  scale_x_continuous(breaks = seq(1992, 2007, 5))

### Part-to-whole
# 100% stacked bar chart (we interested in proportion rather than actual population)
options(scipen = 999) 
gapminder %>% filter(year >= 1992) %>% 
  group_by(year, continent) %>% 
  summarize(totalpop = sum(pop)) %>% 
  ggplot(aes(x = year, y = totalpop, fill = continent)) + 
  geom_col(alpha = 0.8, position = "fill") + 
  labs(title = "Total population by continent", 
       x = "", y = "Population") + 
  scale_x_continuous(breaks = seq(1992, 2007, 5)) # Manually change x breaks behaviour cuz those of ggplot not so good

# tree map
# Diff from heat map, as in heat map all got same size of rectangle and cont variable mapped to color. Heat map can show values over years but tree map cannot
# In tree map continuous variable mapped to size and color: good to show heirichal structure ( group by subgroup/continent)
library(treemapify)
gapminder %>% filter(year== 2007) %>% 
  ggplot(aes(area = gdpPercap, fill = continent, 
             label = country, subgroup = continent)) + 
  geom_treemap() + 
  geom_treemap_subgroup_border(color = "black") + # Borders for the subgroups
  geom_treemap_subgroup_text(fontface = "bold", alpha = 0.7, place = "bottomleft") + # Change font and transparency of tree map
  labs(title = "GDP per capita by continent") + 
  theme(legend.position = "none") # Get rid of colour legend cuz not required anymore
# + geom_treemap_text(place = "center) to label each country in the middle

### Ranking
# lollipop chart is an alternative of bar chart that uses a line segment and a dot to represent each data point.
gapminder %>% filter(year== 2007, continent== "Americas") %>% 
  arrange(gdpPercap) %>% 
  mutate(country = factor(country, levels = country)) %>% 
  ggplot(aes(x = gdpPercap, y = country)) + 
  geom_segment(aes(x = 0, xend = gdpPercap, yend = country), 
               color = "gray", lwd = 1) + # State start and end point of line segment (xend,yend) is the coordinate of the end point
  geom_point(size = 3, color = "maroon") + 
  labs(title = "GDP per capia in selected countries, 2007", 
       x = "", y = "") + 
  theme_classic()

### Spatial
library(maps) 
world <- map_data("world") 
df <- gapminder %>% filter(year== 2007) %>% 
  left_join(world, by = c("country" = "region")) # Country is left table and region is right table cannot do other way rounf or r will complaint that cannot find to map
ggplot(df, aes(x = long, y = lat, 
               group = group, fill = lifeExp)) + 
  geom_polygon(color = "white") + 
  scale_fill_continuous(low = "lightblue", high = "steelblue") + # As seen as befoire gg plot doesnt not do intelligent colour mapping for continuous variable. 
  labs(title = "Life expectancy, 2007", fill = "Age (years)") + 
  theme_void() + 
  theme(legend.position = "top")

# USA was not join, so we double check
gapminder %>% anti_join(world, by = c("country" = "region")) %>% 
  distinct(country)

# Recode unmatched country names
gapminder_cleaned <- gapminder %>% 
  mutate(country = 
           recode(country, 
                  "Korea, Rep." = "South Korea", 
                  "United Kingdom" = "UK", 
                  "United States" = "USA", 
                  "Congo, Rep." = "Republic of Congo", 
                  "Slovak Republic" = "Slovakia")) 

# Now we can join the data frames again
df <- gapminder_cleaned %>% filter(year== 2007) %>% 
  left_join(world, by = c("country" = "region"))

# Rerun code
ggplot(df, aes(x = long, y = lat, 
               group = group, fill = lifeExp)) + 
  geom_polygon(color = "white") + 
  scale_fill_continuous(low = "lightblue", high = "steelblue") + # As seen as befoire gg plot doesnt not do intelligent colour mapping for continuous variable. 
  labs(title = "Life expectancy, 2007", fill = "Age (years)") + 
  theme_void() + 
  theme(legend.position = "top")

# apply pre-defined ggplot color palettes for better consistency and accessibility.
# scale_fill_viridis_c() provides a colorblind-friendly gradient.
# scale_fill_distiller() provides a range of built-in palettes.
# Use either continuous or distinct colour pallettes

ggplot(df, aes(x = long, y = lat, 
               group = group, fill = lifeExp)) + 
  geom_polygon(color = "white") + 
  scale_fill_viridis_c(option = "viridis", direction =-1) + # Direction negative 1 so that we cap colour more intuitively for continuous variable
  labs(title = "Life expectancy, 2007", fill = "Age (years)") + 
  theme_void() + 
  theme(legend.position = "top")

ggplot(df, aes(x = long, y = lat, 
               group = group, fill = lifeExp)) + 
  geom_polygon(color = "white") + 
  scale_fill_distiller(palette = "YlGnBu", direction = 1) + # Direction 1 so that we cap colour more intuitively for continuous variable
  labs(title = "Life expectancy, 2007", fill = "Age (years)") + 
  theme_void() + 
  theme(legend.position = "top")

```

### Case study

```{r}
# By default R do not accept numbers as the variable name so adds X

# Graph 1
df1 <- read.csv("../data/wk13_streamingUS.csv")
df1_tidy <- df1 %>% 
  pivot_longer(X2020:X2021, names_to = "year", values_to = "share") %>%
  mutate(year = str_sub(year, 2, 5),
         share1 = paste0(share, "%")) # Add percent behind the values 
glimpse(df1_tidy)

ggplot(df1_tidy, aes(x = service, y = share, fill = year)) + 
  geom_col(position = position_dodge(0.9), alpha = 0.7) + # Dodge so that can split the bars  
  geom_text(aes(label = share1), size = 3, 
            position = position_dodge(0.9), vjust = "bottom") + # vjust ensure that label aligned to bottom of bar
  labs(title = "US Streaming Market Share", 
       x = "", y = "Market share (%)", fill = "Year") + 
  theme(plot.title = element_text(hjust = 0.5, size = 20), 
        legend.position = "top")

# Graph 2
df1_growth <- df1_tidy %>%  
  group_by(service) %>% 
  mutate(lag_share = lag(share, order_by = year),
         growth = share - lag_share) %>% # Calculate growth
  na.omit() %>%
  select(-share,-lag_share) # Un-select variables

ggplot(df1_growth, aes(x = fct_reorder(service, growth), y = growth)) + # Order service by growth variable
  geom_col(aes(fill = growth > 0)) + 
  scale_fill_manual(values = c("firebrick", "gray25")) + 
  coord_flip() + # So the x and y flip
  labs(title = "Changes in US Streaming Market Share in 2021", 
       subtitle = "Growth vs loss compared to 2020", 
       fill = "Growth > 0", x = "", y = "")

# graph 3
filepath <- "../data/wk13_personal_consumption_expenditures.xlsx"
df3 <- read_excel(filepath) %>% 
  filter(`Sub-Category` == "Cereals") %>% # Filter to keep only cereals, add `` as got dash in the name
  mutate(amount_bn =`Millions of Dollars`/1000) %>% # add `` as got space in the name
  rename(subcategory = 2, month = 3) %>% # give position of columns (another way rather than calling the actual name more convenient)
  select(subcategory, month, amount_bn)
head(df3, 3)

ggplot(df3, aes(x = month, y = amount_bn)) + 
  geom_line(linewidth = 2) + 
  geom_area(fill = "steelblue", alpha = 0.8) + 
  labs(title = "Well that sure was unusual", x = "", y = "", 
       subtitle = "Monthly consumption expenditure on cereals", 
       caption = "Source: US Bureau of Economic Analysis")

# graph 4 focus on 2020

df3_new <- df3 %>% 
  mutate(month = as_date(month), year = year(month), 
         month_abb = month(month, label = TRUE, abbr = TRUE)) %>% # month becomes ordered factor varibale which ggplot will reocgnise and plot instead of alphabetically
  filter(year %in% c(2019, 2020)) %>% 
  pivot_wider(id_cols = month_abb, 
              names_from = year, values_from = amount_bn)
head(df3_new)

ggplot(df3_new, aes(x = month_abb)) + 
  geom_segment(aes(xend = month_abb, y =`2019` 
                   , yend =`2020`), 
               color = "darkgrey") + 
  geom_point(aes(y =`2019`), color = "darkgrey", size = 5) + 
  geom_point(aes(y =`2020`), color = "maroon", size = 5) + 
  labs(title = "Consumption of Cereals during the Pandemic", 
       x = "" , y = "", 
       subtitle = "Monthly expenditure in 2019 (grey) and 2020 (red)", 
       caption = "Source: US Bureau of Economic Analysis")

# Graph 5
filepath <- "../data/wk13_time-spent-with-relationships-by-age-us.csv"
df4 <- read.csv(filepath)%>% 
  rename(age = Year, 
         "Alone" = 4, # Rename by calling position here very conveninent
         "With friends" = 5, 
         "With children" = 6, 
         "With parents" = 7, 
         "With partner" = 8, 
         "With coworkers" = 9) %>% 
  pivot_longer(Alone:`With coworkers`, 
               names_to = "Category", values_to = "Time") %>% 
  filter(age <= 80) 

df4_text <- df4 %>% filter(age== 80)# Remove last part

p4 <- ggplot(df4, aes(x = age, y = Time, color = Category)) + 
  geom_point() + geom_line() + 
  geom_text(data = df4_text, aes(label = Category), 
            hjust = "left", nudge_x = 1, size = 2.3) + # Adjust text posiiton
  scale_x_continuous(breaks = seq(20, 80, 10), limits = c(15, 85)) + # Increase limits so that entire text shows up on the right
  guides(color = "none")+ 
  labs(x = "Age", y = "", 
       title = "Who Americans spent their time with, by age", 
       subtitle = "Average time spent with others, minutes per day")
p4

# Continuous color palette use for continuous data
# Qualitative color palette, use for categorical variable or field
# Diverging color palette, use if have useful median value
p4 + scale_color_brewer(type = "qual", palette = "Dark2")
```
